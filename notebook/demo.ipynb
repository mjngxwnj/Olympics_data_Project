{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5254b23-3e6f-4223-ac9c-3eebe962c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, split, regexp_replace, to_date, length, concat, substring\n",
    "from contextlib import contextmanager\n",
    "#create spark Session\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local'):\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(appName)\n",
    "    conf.setMaster(master)\n",
    "    conf.set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.executor.cores\", \"2\")\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "    print(f\"Successfully create SparkSession with app name: {appName}, master: {master}\\n\")\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "#input HDFS function\n",
    "def upload_HDFS(dataFrame: DataFrame, table_name: str, HDFS_path: str) -> None:\n",
    "    print(f'''Starting upload file \"{table_name}\" into {HDFS_path}...''')\n",
    "    #check types of parameters\n",
    "    if not isinstance(dataFrame, DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    if not isinstance(table_name, str):\n",
    "        raise TypeError(\"table name must be a string!\")\n",
    "    if not HDFS_path.startswith(\"hdfs://namenode:9000/\"):\n",
    "        raise TypeError('HDFS path must start with \"hdfs://namenode:9000/\"')\n",
    "    #upload data\n",
    "    dataFrame.write.parquet(HDFS_path, mode = 'overwrite')\n",
    "    print(\"========================================================\")\n",
    "    print(f'''Successfully upload \"{table_name}\" into {HDFS_path}.''')\n",
    "    print(\"========================================================\")\n",
    "\n",
    "#read file from HDFS function\n",
    "def read_HDFS(spark: SparkSession, HDFS_path: str) -> DataFrame:\n",
    "    print(f\"Starting read file from {HDFS_path}.\")\n",
    "    #check parameters\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a Spark Session!\")\n",
    "    if not HDFS_path.startswith(\"hdfs://namenode:9000/\"):\n",
    "        raise TypeError('HDFS path must start with \"hdfs://namenode:9000/\"')\n",
    "    #read file\n",
    "    data = spark.read.parquet(HDFS_path, header = True)\n",
    "    return data\n",
    "\n",
    "#set spark connection with snowflake data warehouse\n",
    "@contextmanager\n",
    "def get_snowflake_sparkSession(appName: str, master: str = 'local'):\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(appName)\n",
    "    conf.setMaster(master)\n",
    "    conf.set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.executor.cores\", \"2\") \\\n",
    "        .set(\"spark.jars\",\"/opt/jars/snowflake-jdbc-3.19.0.jar, \\\n",
    "                           /opt/jars/spark-snowflake_2.12-2.12.0-spark_3.4.jar\")\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "    print(f\"Successfully create SparkSession for Snowflake with app name: {appName}, master: {master}\\n\")\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "#default config for snowflake\n",
    "sfOptions_default = {\n",
    "    \"sfURL\": \"https://ae58556.ap-southeast-1.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"HUYNHTHUAN\",\n",
    "    \"sfPassword\": \"Thuan0355389551\",\n",
    "    \"sfDatabase\": \"OLYMPICS_DB\",\n",
    "    \"sfSchema\": \"OLYMPICS_SCHEMA\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "#load data from hdfs to snowflake data warehouse\n",
    "def load_snowflake(dataFrame: DataFrame, table_name: str, sfOptions: dict = sfOptions_default):\n",
    "    print(f'''Starting upload {table_name} into snowflake...''')\n",
    "    #check parameters\n",
    "    if not isinstance(dataFrame, DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    if not isinstance(table_name, str):\n",
    "        raise TypeError(\"table name must be a string!\")\n",
    "    #upload data\n",
    "    dataFrame.write \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**sfOptions) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(\"========================================================\")\n",
    "    print(f'''Successfully upload \"{table_name}\" into SnowFlake.''')\n",
    "    print(\"========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b740beb-ba10-42e6-ab02-89945136e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, \n",
    "                               IntegerType, FloatType, ArrayType, \n",
    "                               DateType, ByteType, TimestampType)\n",
    "def get_schema(table_name):\n",
    "    '''\n",
    "        Create schema for athletes table\n",
    "    '''\n",
    "    #list of columns containing string type\n",
    "    cols = ['code', 'name', 'name_short', 'name_tv',\n",
    "            'gender', 'function', 'country_code',\n",
    "            'country', 'country_full', 'nationality', \n",
    "            'nationality_full', 'nationality_code']\n",
    "    \n",
    "    cols2 = ['birth_place', 'birth_country', 'residence_place', \n",
    "             'residence_country', 'nickname', 'hobbies', 'occupation', \n",
    "             'education', 'family']\n",
    "    \n",
    "    cols3 = ['coach', 'reason', 'hero', 'influence', 'philosophy', \n",
    "             'sporting_relatives', 'ritual', 'other_sports']\n",
    "    #create schema for athletes table\n",
    "    athletes_schema  = [StructField(col, StringType(), True) for col in cols]\n",
    "    athletes_schema += [StructField('height', IntegerType(), True),\n",
    "                       StructField('weight', IntegerType(), True),\n",
    "                       StructField('disciplines', ArrayType(StringType(),True), True),\n",
    "                       StructField('events', ArrayType(StringType(),True)),\n",
    "                       StructField('birth_date', DateType(), True)]\n",
    "    athletes_schema += [StructField(col, StringType(), True) for col in cols2]\n",
    "    athletes_schema += [StructField('lang', ArrayType(StringType(),True),True)]\n",
    "    athletes_schema += [StructField(col, StringType(), True) for col in cols3]\n",
    "    athletes_schema  = StructType(athletes_schema)\n",
    "\n",
    "    '''\n",
    "        Create schema for events table\n",
    "    '''\n",
    "    #events table has a suitable schema, so ignore it\n",
    "    events_schema = None\n",
    "    \n",
    "    '''\n",
    "        Create schema for medallists table \n",
    "    '''\n",
    "    medallists_schema = [StructField(\"medal_date\", DateType(), True),\n",
    "                         StructField(\"medal_type\", StringType(), True),\n",
    "                         StructField(\"medal_code\", ByteType(), True)]\n",
    "    \n",
    "    #list of columns containing string type\n",
    "    cols = ['name', 'gender', 'country', 'country_code', 'nationality','team', \n",
    "            'team_gender', 'discipline', 'event', 'event_type', 'url_event']\n",
    "    \n",
    "    medallists_schema += [StructField(col, StringType(), True) for col in cols]\n",
    "    medallists_schema += [StructField(\"birth_date\", DateType(), True),\n",
    "                          StructField(\"code\", StringType(), True)]\n",
    "    medallists_schema  = StructType(medallists_schema)\n",
    "\n",
    "    '''\n",
    "        Create schema for medals table\n",
    "    '''\n",
    "    medals_schema = [StructField(\"medal_type\", StringType(),True),\n",
    "                     StructField(\"medal_code\", ByteType(),True),\n",
    "                     StructField(\"medal_date\", DateType(), True)]\n",
    "    \n",
    "    #list of columns containing string type\n",
    "    cols = ['name', 'country_code', 'gender', 'discipline',\n",
    "             'event', 'event_type', 'url_event', 'code']\n",
    "    \n",
    "    medals_schema += [StructField(col, StringType(), True) for col in cols]\n",
    "    medals_schema  = StructType(medals_schema)\n",
    "\n",
    "    '''\n",
    "        Create schema for schedules table\n",
    "    '''\n",
    "    schedules_schema = [StructField(\"start_date\", TimestampType(), True),\n",
    "                        StructField(\"end_date\", TimestampType(), True),\n",
    "                        StructField(\"day\", DateType(), True),\n",
    "                        StructField(\"status\", StringType(), True),\n",
    "                        StructField(\"discipline\", StringType(), True),\n",
    "                        StructField(\"discipline_code\", StringType(), True),\n",
    "                        StructField(\"event\", StringType(), True),\n",
    "                        StructField(\"event_medal\", IntegerType(), True)]\n",
    "    \n",
    "    #list of columns containing string type \n",
    "    cols = ['phase', 'gender', 'event_type', 'venue', \n",
    "            'venue_code', 'location_description', 'location_code']\n",
    "    \n",
    "    schedules_schema += [StructField(col, StringType(), False) for col in cols]\n",
    "    schedules_schema += [StructField(\"url\", StringType(), True)]\n",
    "    schedules_schema  = StructType(schedules_schema)\n",
    "\n",
    "    '''\n",
    "        Create schema for schedules_preliminary schemas\n",
    "\n",
    "    '''\n",
    "    schedules_pre_schema = [StructField(\"date_start_utc\", TimestampType(), True),\n",
    "                            StructField(\"date_end_utc\", TimestampType(), True),\n",
    "                            StructField(\"estimated\", StringType(), True),\n",
    "                            StructField(\"estimated_start\", StringType(), True),\n",
    "                            StructField(\"start_text\", StringType(), True),\n",
    "                            StructField(\"medal\", IntegerType(), True),\n",
    "                            StructField(\"venue_code\", StringType(), True),\n",
    "                            StructField(\"description\", StringType(), True)]\n",
    "    \n",
    "    #list of columns containing string type - can be nullable\n",
    "    cols = ['venue_code_other', 'discription_other', \n",
    "            'team_1_code', 'team_1', 'team_2_code', 'team_2']\n",
    "    \n",
    "    schedules_pre_schema += [StructField(col, StringType(), True) for col in cols]\n",
    "    schedules_pre_schema += [StructField(\"tag\", StringType(), True),\n",
    "                             StructField(\"sport\", StringType(), True),\n",
    "                             StructField(\"sport_code\", StringType(), True),\n",
    "                             StructField(\"sport_url\", StringType(), True)]\n",
    "    schedules_pre_schema  = StructType(schedules_pre_schema)\n",
    "\n",
    "    '''\n",
    "        Create schema for teams table\n",
    "    '''\n",
    "    #list of column containing string type\n",
    "    cols = ['code', 'team', 'team_gender', 'country', 'country_full', \n",
    "            'country_code', 'discipline', 'disciplines_code', 'events']\n",
    "    \n",
    "    teams_schema  = [StructField(col, StringType(), True) for col in cols]\n",
    "    teams_schema += [StructField(\"athletes\", ArrayType(StringType(),True)),\n",
    "                     StructField(\"coaches\", ArrayType(StringType(),True)),\n",
    "                     StructField(\"athletes_codes\",ArrayType(StringType(),True)),\n",
    "                     StructField(\"num_athletes\",IntegerType(),True),\n",
    "                     StructField(\"coaches_codes\",ArrayType(StringType(),True)),\n",
    "                     StructField(\"num_coaches\",IntegerType(),True)]\n",
    "    teams_schema = StructType(teams_schema)\n",
    "\n",
    "    '''\n",
    "        Create schema for torch_route table\n",
    "    '''\n",
    "    torch_route_schema = [StructField(\"title\", StringType(), True),\n",
    "                          StructField(\"city\", StringType(), True),\n",
    "                          StructField(\"date_start\", TimestampType(), True),\n",
    "                          StructField(\"date_end\", TimestampType(), True),\n",
    "                          StructField(\"tag\", StringType(), True),\n",
    "                          StructField(\"url\", StringType(), True),\n",
    "                          StructField(\"stage_number\", IntegerType(), True)]\n",
    "    torch_route_schema = StructType(torch_route_schema)\n",
    "\n",
    "    '''\n",
    "        Create schema for venues table\n",
    "    '''\n",
    "\n",
    "    venues_schema = [StructField(\"venue\", StringType(), True),\n",
    "                     StructField(\"sports\", ArrayType(StringType(),True),True),\n",
    "                     StructField(\"date_start\", TimestampType(), True),\n",
    "                     StructField(\"date_end\", TimestampType(), True),\n",
    "                     StructField(\"tag\", StringType(), True),\n",
    "                     StructField(\"url\", StringType(), True)]\n",
    "    venues_schema = StructType(venues_schema)\n",
    "\n",
    "    #create dict for mapping schema\n",
    "    schema = {\n",
    "        'athletes' : athletes_schema,\n",
    "        'events' : events_schema,\n",
    "        'medallists' : medallists_schema,\n",
    "        'medals' : medals_schema,\n",
    "        'schedules_preliminary' : schedules_pre_schema,\n",
    "        'schedules' : schedules_schema,\n",
    "        'teams' : teams_schema,\n",
    "        'torch_route' : torch_route_schema,\n",
    "        'venues' : venues_schema\n",
    "    }\n",
    "\n",
    "    return schema[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ff91fda-a51a-4026-84ec-8a15782aec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Bronze task starts!========================\n",
      "Successfully create SparkSession with app name: bronze_task_spark, master: local\n",
      "\n",
      "Starting upload file \"athletes\" into hdfs://namenode:9000/datalake/bronze_storage//athletes/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"athletes\" into hdfs://namenode:9000/datalake/bronze_storage//athletes/.\n",
      "========================================================\n",
      "Starting upload file \"events\" into hdfs://namenode:9000/datalake/bronze_storage//events/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"events\" into hdfs://namenode:9000/datalake/bronze_storage//events/.\n",
      "========================================================\n",
      "Starting upload file \"medallists\" into hdfs://namenode:9000/datalake/bronze_storage//medallists/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"medallists\" into hdfs://namenode:9000/datalake/bronze_storage//medallists/.\n",
      "========================================================\n",
      "Starting upload file \"medals\" into hdfs://namenode:9000/datalake/bronze_storage//medals/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"medals\" into hdfs://namenode:9000/datalake/bronze_storage//medals/.\n",
      "========================================================\n",
      "Starting upload file \"schedules_preliminary\" into hdfs://namenode:9000/datalake/bronze_storage//schedules_preliminary/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"schedules_preliminary\" into hdfs://namenode:9000/datalake/bronze_storage//schedules_preliminary/.\n",
      "========================================================\n",
      "Starting upload file \"schedules\" into hdfs://namenode:9000/datalake/bronze_storage//schedules/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"schedules\" into hdfs://namenode:9000/datalake/bronze_storage//schedules/.\n",
      "========================================================\n",
      "Starting upload file \"teams\" into hdfs://namenode:9000/datalake/bronze_storage//teams/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"teams\" into hdfs://namenode:9000/datalake/bronze_storage//teams/.\n",
      "========================================================\n",
      "Starting upload file \"torch_route\" into hdfs://namenode:9000/datalake/bronze_storage//torch_route/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"torch_route\" into hdfs://namenode:9000/datalake/bronze_storage//torch_route/.\n",
      "========================================================\n",
      "Starting upload file \"venues\" into hdfs://namenode:9000/datalake/bronze_storage//venues/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"venues\" into hdfs://namenode:9000/datalake/bronze_storage//venues/.\n",
      "========================================================\n",
      "========================Bronze task finishes!========================\n"
     ]
    }
   ],
   "source": [
    "#task\n",
    "def bronze_task(tables: list, HDFS_path: str):\n",
    "    with get_sparkSession(\"bronze_task_spark\", \"local\") as spark:\n",
    "        df = None\n",
    "        for table_name in tables:\n",
    "            '''\n",
    "                Note that we need to preprocess data for athletes,\n",
    "                teams and venues table before applying the schema\n",
    "            '''\n",
    "            if table_name == 'athletes':\n",
    "                #read data from csv file\n",
    "                data = spark.read.csv(\"/opt/data/athletes.csv\", header = True)\n",
    "                #replace unnecessary character \n",
    "                data = data.withColumn(\"disciplines\", regexp_replace(\"disciplines\", \"[\\[\\]']\", \"\")) \\\n",
    "                           .withColumn(\"events\", regexp_replace(\"events\",\"[\\[\\]']\",\"\")) \n",
    "                data = data.withColumn(\"disciplines\", split(data[\"disciplines\"],\",\")) \\\n",
    "                           .withColumn(\"events\", split(data[\"events\"],\",\")) \\\n",
    "                           .withColumn(\"height\", col(\"height\").cast(\"int\")) \\\n",
    "                           .withColumn(\"weight\", col(\"weight\").cast(\"int\")) \\\n",
    "                           .withColumn(\"birth_date\", to_date(col(\"birth_date\"), \"yyyy-mm-dd\")) \\\n",
    "                           .withColumn(\"lang\", split(data[\"lang\"],\",\"))\n",
    "                #create dataFrame\n",
    "                df = spark.createDataFrame(data.rdd, schema = get_schema(table_name))\n",
    "            elif table_name == \"teams\":\n",
    "                #read data from csv file\n",
    "                data = spark.read.csv(\"/opt/data/teams.csv\", header = True)\n",
    "                #replace unnecessary characters\n",
    "                data = data.withColumn(\"athletes\", regexp_replace(\"athletes\",\"[\\[\\]']\",\"\")) \\\n",
    "                        .withColumn(\"coaches\", regexp_replace(\"coaches\",\"[\\[\\]']\",\"\")) \\\n",
    "                        .withColumn(\"athletes_codes\", regexp_replace(\"athletes_codes\",\"[\\[\\]']\",\"\")) \\\n",
    "                        .withColumn(\"coaches_codes\", regexp_replace(\"coaches_codes\",\"[\\[\\]']\",\"\"))\n",
    "                #transform data type\n",
    "                data = data.withColumn(\"athletes\", split(data[\"athletes\"],\",\")) \\\n",
    "                        .withColumn(\"coaches\", split(data[\"coaches\"],\",\")) \\\n",
    "                        .withColumn(\"athletes_codes\", split(data[\"athletes_codes\"],\",\")) \\\n",
    "                        .withColumn(\"coaches_codes\", split(data[\"coaches_codes\"],\",\")) \\\n",
    "                        .withColumn(\"num_athletes\", col(\"num_athletes\").cast(\"int\")) \\\n",
    "                        .withColumn(\"num_coaches\", col(\"num_coaches\").cast(\"int\"))\n",
    "                #create dataFrame\n",
    "                df = spark.createDataFrame(data.rdd, schema = get_schema(table_name))\n",
    "            elif table_name == 'venues':\n",
    "                #read data from csv file\n",
    "                data = spark.read.csv(\"/opt/data/venues.csv\", header = True)\n",
    "                #replace unnecessary characters\n",
    "                data = data.withColumn(\"sports\", regexp_replace(\"sports\",\"[\\[\\]']\",\"\"))\n",
    "                data = data.withColumn(\"sports\", split(data[\"sports\"],\",\")) \\\n",
    "                        .withColumn(\"date_start\", col(\"date_start\").cast(\"timestamp\")) \\\n",
    "                        .withColumn(\"date_end\", col(\"date_end\").cast(\"timestamp\"))\n",
    "                #create dataFrame\n",
    "                df = spark.createDataFrame(data.rdd, schema = get_schema(table_name)) \n",
    "            else:\n",
    "                df = spark.read.csv(f\"/opt/data/{table_name}.csv\",header = True, schema = get_schema(table_name))\n",
    "            #After reading csv files or preprocessing, we load data into HDFS\n",
    "            upload_HDFS(df, table_name, HDFS_path + f'/{table_name}/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #List all tables\n",
    "    tables = ['athletes', 'events', 'medallists', \n",
    "            'medals', 'schedules_preliminary', \n",
    "            'schedules', 'teams', 'torch_route', 'venues']\n",
    "    #HDFS path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/\"\n",
    "    print(\"=========================Bronze task starts!========================\")\n",
    "    bronze_task(tables, HDFS_path)\n",
    "    print(\"========================Bronze task finishes!========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71351501-6433-44f8-b19d-036856c67872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, explode_outer\n",
    "#create silver clean classs\n",
    "class Silverlayer:\n",
    "    #init\n",
    "    def __init__(self, df: DataFrame, \n",
    "                 dropna_columns: list = None,\n",
    "                 columns_dropDuplicates: list = None,\n",
    "                 columns_drop: list = None, \n",
    "                 columns_rename: dict = None, \n",
    "                 nested_columns: list = None,\n",
    "                 missval_columns: dict = None,\n",
    "                 ):\n",
    "        '''\n",
    "            Initializes processing task\n",
    "        '''\n",
    "        #firstly, check all types of parameters \n",
    "        if df is not None and not isinstance(df, DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame\")\n",
    "        if columns_dropDuplicates is not None and not isinstance(columns_dropDuplicates, list):\n",
    "            raise TypeError(\"columns_dropDuplicates must be a list\")\n",
    "        if columns_drop is not None and not isinstance(columns_drop, list):\n",
    "            raise TypeError(\"columns_drop must be a list\")\n",
    "        if columns_rename is not None and not isinstance(columns_rename, dict):\n",
    "            raise TypeError(\"columns_rename must be a dict\")\n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"nested_columns must be a list\")\n",
    "        if missval_columns is not None and not isinstance(missval_columns, dict):\n",
    "            raise TypeError(\"columns_null must be a dict\")\n",
    "        if dropna_columns is not None and not isinstance(dropna_columns, list):\n",
    "            raise TypeError(\"dropna_columns must be a list\")\n",
    "        #set value for class attrs\n",
    "        #data frame\n",
    "        self.df = df\n",
    "        #list of columns to apply the drop duplicate function\n",
    "        self.columns_dropDuplicates = columns_dropDuplicates\n",
    "        #list of columns to drop\n",
    "        self.columns_drop = columns_drop\n",
    "        #dict containing old name & new name\n",
    "        self.columns_rename = columns_rename\n",
    "        #list of columns that need to handle nested structures\n",
    "        self.nested_columns = nested_columns\n",
    "        #dict containing columns to check & a value to apply for nulls\n",
    "        self.missval_columns = missval_columns \n",
    "        #list of columns to apply drop na function\n",
    "        self.dropna_columns = dropna_columns\n",
    "    def drop_na(self, df: DataFrame, dropna_columns: list):\n",
    "        '''\n",
    "            Drop rows containing na values\n",
    "        '''\n",
    "        self.df = df.dropna(how = 'all', subset = dropna_columns)\n",
    "\n",
    "    def drop_duplicates(self, df: DataFrame, columns_dropDuplicates: list):\n",
    "        '''\n",
    "            Drop duplicates based on specified columns\n",
    "        '''\n",
    "        self.df = df.dropDuplicates(columns_dropDuplicates)\n",
    "\n",
    "    def drop_columns(self, df: DataFrame, columns_drop: list):\n",
    "        '''\n",
    "            Drop unnecessary columns\n",
    "        '''\n",
    "        self.df = df.drop(*columns_drop)\n",
    "\n",
    "    def rename_columns(self, columns_rename: dict):\n",
    "        '''\n",
    "            Rename columns\n",
    "        '''\n",
    "        for old_name, new_name in columns_rename.items():\n",
    "            self.df = self.df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    def handle_nested(self, nested_columns: list):\n",
    "        '''\n",
    "            Handle nested columns \n",
    "        '''\n",
    "        for col in nested_columns:\n",
    "            self.df = self.df.withColumn(col, explode_outer(col))\n",
    "\n",
    "    def handle_missing(self, missval_columns: dict):\n",
    "        '''\n",
    "            Handle missing values\n",
    "        '''\n",
    "        for col, value in missval_columns.items():\n",
    "            self.df = self.df.fillna(value = value, subset = col)\n",
    "\n",
    "    def process(self) -> DataFrame:\n",
    "        '''\n",
    "            Process based on all parameters\n",
    "        '''\n",
    "        self.drop_na(self.df, self.dropna_columns)\n",
    "        self.drop_duplicates(self.df, self.columns_dropDuplicates)\n",
    "\n",
    "        if self.columns_drop:\n",
    "            self.drop_columns(self.df, self.columns_drop)\n",
    "        \n",
    "        if self.columns_rename:\n",
    "            self.rename_columns(self.columns_rename)\n",
    "\n",
    "        if self.nested_columns:\n",
    "            self.handle_nested(self.nested_columns)\n",
    "        \n",
    "        if self.missval_columns:\n",
    "            self.handle_missing(self.missval_columns)\n",
    "\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "998ae512-7c94-492e-ac72-e79f455d13f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Silver task starts!========================\n",
      "Successfully create SparkSession with app name: silver_task_spark, master: local\n",
      "\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/athletes.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/events.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/medallists.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/medals.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/schedules_preliminary.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/schedules.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/teams.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/torch_route.\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/venues.\n",
      "Starting upload file \"athletes_silver\" into hdfs://namenode:9000/datalake/silver_storage/athletes_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"athletes_silver\" into hdfs://namenode:9000/datalake/silver_storage/athletes_silver/.\n",
      "========================================================\n",
      "Starting upload file \"events_silver\" into hdfs://namenode:9000/datalake/silver_storage/events_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"events_silver\" into hdfs://namenode:9000/datalake/silver_storage/events_silver/.\n",
      "========================================================\n",
      "Starting upload file \"medallists_silver\" into hdfs://namenode:9000/datalake/silver_storage/medallists_silver/...\n",
      "========================================================\n",
      "Successfully upload \"medallists_silver\" into hdfs://namenode:9000/datalake/silver_storage/medallists_silver/.\n",
      "========================================================\n",
      "Starting upload file \"medals_silver\" into hdfs://namenode:9000/datalake/silver_storage/medals_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"medals_silver\" into hdfs://namenode:9000/datalake/silver_storage/medals_silver/.\n",
      "========================================================\n",
      "Starting upload file \"schedules_preliminary_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_preliminary_silver/...\n",
      "========================================================\n",
      "Successfully upload \"schedules_preliminary_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_preliminary_silver/.\n",
      "========================================================\n",
      "Starting upload file \"schedules_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"schedules_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_silver/.\n",
      "========================================================\n",
      "Starting upload file \"teams_silver\" into hdfs://namenode:9000/datalake/silver_storage/teams_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"teams_silver\" into hdfs://namenode:9000/datalake/silver_storage/teams_silver/.\n",
      "========================================================\n",
      "Starting upload file \"torch_route_silver\" into hdfs://namenode:9000/datalake/silver_storage/torch_route_silver/...\n",
      "========================================================\n",
      "Successfully upload \"torch_route_silver\" into hdfs://namenode:9000/datalake/silver_storage/torch_route_silver/.\n",
      "========================================================\n",
      "Starting upload file \"venues_silver\" into hdfs://namenode:9000/datalake/silver_storage/venues_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"venues_silver\" into hdfs://namenode:9000/datalake/silver_storage/venues_silver/.\n",
      "========================================================\n",
      "========================Silver task finishes!========================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_zip, explode, col\n",
    "'''\n",
    "    Processing tables individually\n",
    "'''\n",
    "#athletes\n",
    "def athletes_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process athletes table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/athletes\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    columns_drop = ['name_short', 'name_tv', 'hobbies', 'occupation', 'education', \n",
    "                    'family', 'coach', 'reason', 'hero', 'influence', 'philosophy', \n",
    "                    'sporting_relatives', 'ritual', 'other_sports']\n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            columns_drop    =  columns_drop,\n",
    "                            columns_rename  = {'code':'athletes_id', 'name':'full_name', 'lang':'language'},\n",
    "                            nested_columns  = ['disciplines', 'events', 'language'],\n",
    "                            missval_columns = {'birth_place':'N/A', \n",
    "                                                'birth_country':'N/A', \n",
    "                                                'nickname': 'N/A',\n",
    "                                                'residence_place':'N/A', \n",
    "                                                'residence_country':'N/A',\n",
    "                                                'nationality':'N/A',\n",
    "                                                'nationality_full':'N/A',\n",
    "                                                'nationality_code':'N/A',\n",
    "                                                'language':'N/A',\n",
    "                                                'height':0, 'weight':0}).process()\n",
    "    return df_silver\n",
    "\n",
    "#events\n",
    "def events_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process events table\n",
    "    '''\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/events\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            columns_dropDuplicates = ['event', 'sport'],\n",
    "                            columns_drop           = ['sport_url', 'tag'], \n",
    "                            columns_rename         = {'sport_code':'id_sport'}).process()\n",
    "    return df_silver\n",
    "\n",
    "#medallist\n",
    "def medallists_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process medallists table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/medallists\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            dropna_columns         = ['name'], \n",
    "                            columns_dropDuplicates = ['name', 'medal_type', 'discipline', 'event'],\n",
    "                            columns_drop           = ['medal_code', 'url_event'],\n",
    "                            columns_rename         = {'name':'full_name', 'code':'athletes_id'},\n",
    "                            missval_columns        = {'team':'N/A','team_gender':'N/A', 'nationality':'N/A'}).process()\n",
    "    return df_silver\n",
    "\n",
    "#medals\n",
    "def medals_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process medals table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/medals\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['name'], \n",
    "                            columns_dropDuplicates = ['name', 'medal_type', 'discipline', 'event'],\n",
    "                            columns_drop           = ['medal_code', 'url_event'],\n",
    "                            columns_rename         = {'name':'full_name', 'code':'athletes_id'}).process()\n",
    "    return df_silver\n",
    "\n",
    "#schedules\n",
    "def schedules_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process schedules table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/schedules\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #upload file\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['event'],\n",
    "                            columns_dropDuplicates = ['event', 'discipline', 'phase'],\n",
    "                            columns_drop           = ['status', 'event_medal', 'url']).process()\n",
    "    \n",
    "    df_silver = df_silver.withColumn('venue', regexp_replace(\"venue\", \"\\\\d\", \"\")) \\\n",
    "           .withColumn('venue', rtrim(col('venue'))) \\\n",
    "           .withColumn('venue_code', regexp_replace(\"venue_code\", \"\\\\d\", \"\")) \\\n",
    "           .withColumn('venue_code', rtrim(col('venue_code')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Chateauroux Shooting Ctr', 'Chateauroux Shooting Centre').otherwise(col('venue'))) \n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Nautical St - Flat water', 'Vaires-sur-Marne Nautical Stadium').otherwise(col('venue')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'BMX Stadium', 'Saint-Quentin-en-Yvelines BMX Stadium').otherwise(col('venue')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Champ-de-Mars Arena', 'Champ de Mars Arena').otherwise(col('venue')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Le Bourget Climbing Venue', 'Le Bourget Sport Climbing Venue').otherwise(col('venue')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Nautical St - White water', 'Vaires-sur-Marne Nautical Stadium').otherwise(col('venue')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Roland-Garros Stadium', 'Stade Roland-Garros').otherwise(col('venue')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Le Golf National', 'Golf National').otherwise(col('venue')))\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'National Velodrome', 'Saint-Quentin-en-Yvelines Velodrome').otherwise(col('venue')))         \n",
    "    return df_silver\n",
    "\n",
    "#schedules_preliminary\n",
    "def schedules_pre_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process schedules preliminary table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/schedules_preliminary\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['team_1_code', 'team_2_code', 'team_1', 'team_2'],\n",
    "                            columns_dropDuplicates = ['team_1_code', 'team_2_code', 'sport'],\n",
    "                            columns_drop           = ['estimated', 'estimated_start', 'start_text', \n",
    "                                                    'medal', 'sport_url', 'tag'],\n",
    "                            missval_columns        = {'venue_code':'N/A', 'venue_code_other':'N/A',\n",
    "                                                    'discription_other':'N/A'}).process()\n",
    "    return df_silver\n",
    "\n",
    "#teams\n",
    "def teams_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process teams table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/teams\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    #first, we need to merge array athletes and athletes_code to handle nested structure\n",
    "    df = df.withColumn('athletes_id_merge', arrays_zip('athletes','athletes_codes'))\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['code', 'team'],\n",
    "                            columns_dropDuplicates = ['code', 'team'],\n",
    "                            columns_drop           = ['coaches', 'coaches_codes', 'num_coaches', \n",
    "                                                      'athletes', 'athletes_codes'],\n",
    "                            columns_rename         = {'events':'event', 'code':'team_id', 'team':'team_name'},\n",
    "                            missval_columns        = {'event':'Default'}).process()\n",
    "    #After silver processing, we process nested structure\n",
    "    df_silver = df_silver.withColumn('athletes_id_merge', explode('athletes_id_merge'))\n",
    "    df_silver = df_silver.withColumn('athletes', col('athletes_id_merge.athletes')) \\\n",
    "                         .withColumn('athletes_id', col('athletes_id_merge.athletes_codes'))\n",
    "    df_silver = df_silver.drop('athletes_id_merge')\n",
    "    return df_silver\n",
    "\n",
    "#torch_route\n",
    "def torch_route_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process torch route table\n",
    "    '''\n",
    "    #read filey\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/torch_route\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            dropna_columns         = ['title'],\n",
    "                            columns_dropDuplicates = ['title'],\n",
    "                            columns_drop           = ['tag', 'url'],\n",
    "                            missval_columns        = {'city':'N/A', 'stage_number':0}).process()\n",
    "    return df_silver\n",
    "\n",
    "#venues\n",
    "def venues_silver(spark: SparkSession) -> DataFrame:\n",
    "    '''\n",
    "        Process venues table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/venues\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['venue'],\n",
    "                            columns_dropDuplicates = ['venue'],\n",
    "                            columns_drop           = ['tag', 'url'],\n",
    "                            columns_rename         = {'sports':'sport'},\n",
    "                            nested_columns         = ['sport'],\n",
    "                            missval_columns        = {'date_start':'N/A', 'date_end':'N/A'}).process()\n",
    "    return df_silver\n",
    "\n",
    "def silver_task(HDFS_path: str):\n",
    "    with get_sparkSession(\"silver_task_spark\", \"local\") as spark:\n",
    "        #create dict\n",
    "        silver_layer_dict = {}\n",
    "        silver_layer_dict['athletes_silver']               = athletes_silver(spark)\n",
    "        silver_layer_dict['events_silver']                 = events_silver(spark)\n",
    "        silver_layer_dict['medallists_silver']             = medallists_silver(spark)\n",
    "        silver_layer_dict['medals_silver']                 = medals_silver(spark)\n",
    "        silver_layer_dict['schedules_preliminary_silver']  = schedules_pre_silver(spark)\n",
    "        silver_layer_dict['schedules_silver']              = schedules_silver(spark)\n",
    "        silver_layer_dict['teams_silver']                  = teams_silver(spark)\n",
    "        silver_layer_dict['torch_route_silver']            = torch_route_silver(spark)\n",
    "        silver_layer_dict['venues_silver']                 = venues_silver(spark)\n",
    "        #load data into HDFS\n",
    "        for table_name, data in silver_layer_dict.items():\n",
    "            upload_HDFS(data, table_name, HDFS_path + f'/{table_name}/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #HDFS path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage\"\n",
    "    print(\"=========================Silver task starts!========================\")\n",
    "    silver_task(HDFS_path)\n",
    "    print(\"========================Silver task finishes!========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8af49611-1b1f-46b9-9e03-6e289ce0ecb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m HDFS_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:9000/datalake/silver_storage/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#call \u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msparkSession\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_data_quality_spark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m spark:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[1;32m     31\u001b[0m         check_data(spark, HDFS_path, table)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Check data quality of tables\n",
    "'''\n",
    "def check_data(spark: SparkSession, HDFS_path: str, table_name: str):\n",
    "    '''\n",
    "        Check data of all tables\n",
    "    '''\n",
    "    #read file\n",
    "    df = read_HDFS(spark, HDFS_path + f'/{table_name}/')\n",
    "    print(f'Checking data for table \"{table_name}\"...')\n",
    "    df.show(5, truncate = False)\n",
    "\n",
    "    #count the number of null values in each column of the table\n",
    "    df_checkNull = {col:df.filter(df[col].isNull()).count() for col in df.columns}\n",
    "    for col, num in df_checkNull.items():\n",
    "        print(f'Number of null values in column \"{col}\": {num}')\n",
    "\n",
    "    #print schema to ensure that all data doesn't have nested structure(array type of struct type)\n",
    "    df.printSchema()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #list all tables\n",
    "    tables = ['athletes_silver', 'events_silver', 'medallists_silver', \n",
    "              'medals_silver', 'schedules_preliminary_silver', 'schedules_silver', \n",
    "              'teams_silver', 'torch_route_silver', 'venues_silver']\n",
    "    #path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/\"\n",
    "    #call \n",
    "    with sparkSession(\"check_data_quality_spark\", \"local\") as spark:\n",
    "        for table in tables:\n",
    "            check_data(spark, HDFS_path, table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24ea69-d23f-4fea-b3f5-4811ecf8556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (monotonically_increasing_id, concat, \n",
    "                                   lit, substring, rtrim, when, length)\n",
    "\n",
    "#dim table - medal \n",
    "def dim_medal(spark: SparkSession, HDFS_load):\n",
    "    data = [(1,'Gold Medal'),(2,'Silver Medal'),(3,'Bronze Medal')]\n",
    "    df_gold = spark.createDataFrame(data, schema = \"medal_id int, medal_type string\")\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_medal', HDFS_load + '/dim_medal/')\n",
    "\n",
    "#dim table - discipline\n",
    "def dim_discipline(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/schedules_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #select\n",
    "    df_gold = df.select('discipline_code', 'discipline').distinct() \\\n",
    "                .withColumnRenamed('discipline_code', 'discipline_id') \\\n",
    "                .withColumnRenamed('discipline', 'discipline_type')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_discipline', HDFS_load + '/dim_discipline/')\n",
    "\n",
    "#dim table - event\n",
    "def dim_event(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/events_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df = df.select('event').distinct()\n",
    "    #add column\n",
    "    df = df.withColumn('event_id', monotonically_increasing_id())\n",
    "    df = df.withColumn('event_id', concat(lit('ev'), col('event_id')))\n",
    "    #rename\n",
    "    df_gold = df.withColumnRenamed('event', 'event_type')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_event', HDFS_load + '/dim_event/')\n",
    "    \n",
    "#dim table - country\n",
    "def dim_country(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/athletes_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #select\n",
    "    df_nationality = df.select('country_code', 'country')\n",
    "    df_country = df.select('nationality_code', 'nationality')\n",
    "    #union column\n",
    "    df_gold = df_nationality.union(df_country).distinct()\n",
    "    #rename\n",
    "    df_gold = df_gold.withColumnRenamed('country_code', 'country_id') \\\n",
    "                     .withColumnRenamed('country', 'country_name')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_country', HDFS_load + '/dim_country/')\n",
    "    \n",
    "#fact table - medallist\n",
    "def fact_medallist(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/medals_silver\"\n",
    "    HDFS_discipline_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_discipline\"\n",
    "    HDFS_event_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_event\"\n",
    "    HDFS_team = \"hdfs://namenode:9000/datalake/silver_storage/teams_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df_discipline = read_HDFS(spark, HDFS_discipline_gold)\n",
    "    df_event = read_HDFS(spark, HDFS_event_gold)\n",
    "    df_team = read_HDFS(spark, HDFS_team)\n",
    "    df_team = df_team.select('team_id', 'athletes_id')\n",
    "    \n",
    "    df1 = df.filter(length('athletes_id') == 7)\n",
    "    df1 = df1.select('athletes_id', 'medal_type', 'medal_date', 'discipline', 'event')\n",
    "    df2 = df.filter(length('athletes_id') > 7) \\\n",
    "            .withColumnRenamed('athletes_id', 'athletes_team_id') \n",
    "    df2 = df2.join(df_team, df2['athletes_team_id'] == df_team['team_id'], how = 'left')\n",
    "    df2 = df2.select('athletes_id', 'medal_type', 'medal_date', 'discipline', 'event')\n",
    "    df_full = df1.union(df2).distinct()\n",
    "    #handle table individually\n",
    "    df_full = df_full.withColumn('medallist_id', concat(col('athletes_id'),substring('medal_type',1,1))) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Bronze Medal', '3')) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Silver Medal', '2')) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Gold Medal', '1')) \\\n",
    "           .withColumn('medal_type', col('medal_type').cast('int')) \\\n",
    "           .withColumnRenamed('medal_type', 'medal_id') \\\n",
    "\n",
    "    #joining table\n",
    "    df_full = df_full.join(df_discipline, df_full['discipline'] == df_discipline['discipline_type'], how = 'left')\n",
    "    df_full = df_full.join(df_event, df_full['event'] == df_event['event_type'], how = 'left')\n",
    "    #select\n",
    "    df_gold = df_full.select('medallist_id', 'athletes_id', 'medal_id', 'medal_date', 'discipline_id', 'event_id')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'fact_medallist', HDFS_load + '/fact_medallist/')\n",
    "\n",
    "#dim table - athletes\n",
    "def dim_athletes(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_athletes_path = \"hdfs://namenode:9000/datalake/silver_storage/athletes_silver\"\n",
    "    HDFS_team_path = \"hdfs://namenode:9000/datalake/silver_storage/teams_silver\"\n",
    "    df = read_HDFS(spark, HDFS_athletes_path)\n",
    "    df_team = read_HDFS(spark, HDFS_team_path)\n",
    "    df_team = df_team.select('team_id', 'athletes_id')\n",
    "\n",
    "    #joining table\n",
    "    df = df.join(df_team, on = 'athletes_id', how = 'left')\n",
    "    #select\n",
    "    df = df.select('athletes_id', 'full_name', 'gender', \n",
    "                   'function', 'country_code', 'nationality_code', \n",
    "                   'height', 'weight', 'birth_date', 'team_id').distinct()\n",
    "    #rename\n",
    "    df_gold = df.withColumnRenamed('country_code', 'country_id') \\\n",
    "                .withColumnRenamed('nationality_code', 'nationality_id')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_athletes', HDFS_load + '/dim_athletes/')\n",
    "\n",
    "#dim table - team\n",
    "def dim_team(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/teams_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df_gold = df.select('team_id', 'team_name', 'team_gender').distinct()\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_team', HDFS_load + '/dim_team/')\n",
    "\n",
    "#fact table - schedule & dim table - venue\n",
    "def fact_schedule_dim_venue(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_schedule_path = \"hdfs://namenode:9000/datalake/silver_storage/schedules_silver\"\n",
    "    HDFS_venue_path = \"hdfs://namenode:9000/datalake/silver_storage/venues_silver\"\n",
    "    HDFS_discipline_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_discipline\"\n",
    "    HDFS_event_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_event\"\n",
    "\n",
    "    df = read_HDFS(spark, HDFS_schedule_path)\n",
    "    df_discipline = read_HDFS(spark, HDFS_discipline_gold)\n",
    "    df_event = read_HDFS(spark, HDFS_event_gold)\n",
    "    df_venue = read_HDFS(spark, HDFS_venue_path)\n",
    "    '''\n",
    "        Create fact - schedule table\n",
    "    '''\n",
    "    #preprocess to match name of venue before joining \n",
    "    # df = df.withColumn('venue', regexp_replace(\"venue\", \"\\\\d\", \"\")) \\\n",
    "    #        .withColumn('venue', rtrim(col('venue'))) \\\n",
    "    #        .withColumn('venue_code', regexp_replace(\"venue_code\", \"\\\\d\", \"\")) \\\n",
    "    #        .withColumn('venue_code', rtrim(col('venue_code')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'Chateauroux Shooting Ctr', 'Chateauroux Shooting Centre').otherwise(col('venue'))) \n",
    "    # df = df.withColumn('venue', when(col('venue') == 'Nautical St - Flat water', 'Vaires-sur-Marne Nautical Stadium').otherwise(col('venue')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'BMX Stadium', 'Saint-Quentin-en-Yvelines BMX Stadium').otherwise(col('venue')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'Champ-de-Mars Arena', 'Champ de Mars Arena').otherwise(col('venue')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'Le Bourget Climbing Venue', 'Le Bourget Sport Climbing Venue').otherwise(col('venue')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'Nautical St - White water', 'Vaires-sur-Marne Nautical Stadium').otherwise(col('venue')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'Roland-Garros Stadium', 'Stade Roland-Garros').otherwise(col('venue')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'Le Golf National', 'Golf National').otherwise(col('venue')))\n",
    "    # df = df.withColumn('venue', when(col('venue') == 'National Velodrome', 'Saint-Quentin-en-Yvelines Velodrome').otherwise(col('venue')))                                                                                                                 \n",
    "    #add column\n",
    "    df = df.withColumn('schedule_id', monotonically_increasing_id())\n",
    "    df = df.withColumn('schedule_id', concat(lit('sched'), col('schedule_id'))) \\\n",
    "    #drop column\n",
    "    df = df.drop('event_type')\n",
    "    #joining table\n",
    "    df = df.join(df_discipline, df['discipline'] == df_discipline['discipline_type'], how = 'left')\n",
    "    df = df.join(df_event, df['event'] == df_event['event_type'], how = 'left')\n",
    "    #select\n",
    "    df_gold_schedule = df.select('schedule_id', 'start_date', 'end_date', \n",
    "                        'gender', 'discipline_id', 'phase', 'venue_code', 'event_id') \\\n",
    "                .withColumnRenamed('venue_code', 'venue_id') \\\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold_schedule, 'fact_schedule', HDFS_load + '/fact_schedule/')\n",
    "\n",
    "    '''\n",
    "        Create dim - venue table\n",
    "    '''\n",
    "    #get data\n",
    "    df_venue_join = df.select('venue_code', 'venue').distinct() \n",
    "    df_venue = df_venue.join(df_venue_join, on = 'venue', how = 'left')\n",
    "    #select\n",
    "    df_gold_venue = df_venue.select('venue_code', 'venue', 'date_start', 'date_end')\\\n",
    "                        .withColumnRenamed('venue_code', 'venue_id') \\\n",
    "                        .withColumnRenamed('venue', 'venue_name')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold_venue, 'dim_venue', HDFS_load + '/dim_venue/')\n",
    "\n",
    "#main call\n",
    "def gold_task(spark, HDFS_load):\n",
    "    #all func\n",
    "    dim_medal(spark, HDFS_load)\n",
    "    dim_discipline(spark, HDFS_load)\n",
    "    dim_event(spark, HDFS_load)\n",
    "    dim_country(spark, HDFS_load)\n",
    "    fact_medallist(spark, HDFS_load)\n",
    "    dim_athletes(spark, HDFS_load)\n",
    "    fact_schedule_dim_venue(spark, HDFS_load)\n",
    "    dim_team(spark, HDFS_load)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #hdfs path\n",
    "    HDFS_load = \"hdfs://namenode:9000/datalake/gold_storage\"\n",
    "\n",
    "    print(\"=========================Gold task starts!========================\")\n",
    "    with get_sparkSession(\"gold_task_spark\", \"local\") as spark:\n",
    "        gold_task(spark, HDFS_load)\n",
    "    print(\"========================Gold task finishes!========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968b76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|medal_id|  medal_type|\n",
      "+--------+------------+\n",
      "|       1|  Gold Medal|\n",
      "|       2|Silver Medal|\n",
      "|       3|Bronze Medal|\n",
      "+--------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 15:51:38 WARN ServerConnection$: JDBC 3.19.0 is being used. But the certified JDBC version 3.13.30 is recommended.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"MySparkApp\") \\\n",
    "        .master(\"local[*]\")\\\n",
    "        .config('spark.jars','/opt/jars/snowflake-jdbc-3.19.0.jar, /opt/jars/spark-snowflake_2.12-2.12.0-spark_3.4.jar')\\\n",
    "        .getOrCreate()\n",
    "sfOptions = {\n",
    "    \"sfURL\": \"https://ae58556.ap-southeast-1.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"HUYNHTHUAN\",\n",
    "    \"sfPassword\": \"Thuan0355389551\",\n",
    "    \"sfDatabase\": \"OLYMPICS_DB\",\n",
    "    \"sfSchema\": \"OLYMPICS_SCHEMA\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "df = spark.read.parquet(\"hdfs://namenode:9000/datalake/gold_storage/dim_medal\", header = True)\n",
    "df.show()\n",
    "df.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"DIM_MEDAL\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b82542-76db-4ecf-a0ab-c2ed63361a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully create SparkSession with app name: check, master: local\n",
      "\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/schedules_silver.\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/venues_silver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|venue_code|      venue_schedule|               venue|               sport|         date_start|           date_end|\n",
      "+----------+--------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|       LYO|        Lyon Stadium|        Lyon Stadium|            Football|2024-07-25 17:00:00|2024-08-09 16:00:00|\n",
      "|       CPL|   La Chapelle Arena|   La Chapelle Arena| Rhythmic Gymnastics|2024-07-27 06:30:00|2024-08-10 13:45:00|\n",
      "|       CPL|   La Chapelle Arena|   La Chapelle Arena|           Badminton|2024-07-27 06:30:00|2024-08-10 13:45:00|\n",
      "|       INV|           Invalides|           Invalides|        Cycling Road|2024-07-25 07:30:00|2024-08-04 13:20:00|\n",
      "|       INV|           Invalides|           Invalides|           Athletics|2024-07-25 07:30:00|2024-08-04 13:20:00|\n",
      "|       INV|           Invalides|           Invalides|             Archery|2024-07-25 07:30:00|2024-08-04 13:20:00|\n",
      "|       PDP|    Parc des Princes|    Parc des Princes|            Football|2024-07-25 13:00:00|2024-08-10 18:30:00|\n",
      "|       EIF|Eiffel Tower Stadium|Eiffel Tower Stadium|    Beach Volleyball|2024-07-27 12:00:00|2024-08-10 21:59:00|\n",
      "|       ALX|  Pont Alexandre III|  Pont Alexandre III|           Triathlon|2024-07-30 06:00:00|2024-08-09 08:30:00|\n",
      "|       ALX|  Pont Alexandre III|  Pont Alexandre III|   Marathon Swimming|2024-07-30 06:00:00|2024-08-09 08:30:00|\n",
      "|       ALX|  Pont Alexandre III|  Pont Alexandre III|        Cycling Road|2024-07-30 06:00:00|2024-08-09 08:30:00|\n",
      "|       DEF|Paris La Defense ...|Paris La Defense ...|          Water Polo|2024-07-27 09:00:00|2024-08-11 13:50:00|\n",
      "|       DEF|Paris La Defense ...|Paris La Defense ...|            Swimming|2024-07-27 09:00:00|2024-08-11 13:50:00|\n",
      "|       LBO|Le Bourget Sport ...|Le Bourget Sport ...|      Sport Climbing|2024-08-05 08:00:00|2024-08-10 11:20:00|\n",
      "|       NIC|        Nice Stadium|        Nice Stadium|            Football|2024-07-25 15:00:00|2024-08-31 21:00:00|\n",
      "|       AQC|     Aquatics Centre|     Aquatics Centre|          Water Polo|2024-07-27 09:00:00|2024-08-10 20:00:00|\n",
      "|       AQC|     Aquatics Centre|     Aquatics Centre|              Diving|2024-07-27 09:00:00|2024-08-10 20:00:00|\n",
      "|       AQC|     Aquatics Centre|     Aquatics Centre|   Artistic Swimming|2024-07-27 09:00:00|2024-08-10 20:00:00|\n",
      "|       MAM|    Marseille Marina|    Marseille Marina|             Sailing|2024-07-28 09:00:00|2024-08-08 17:00:00|\n",
      "|        VE|Saint-Quentin-en-...|Saint-Quentin-en-...|  Cycling BMX Racing|2024-08-01 18:00:00|2024-08-02 20:30:00|\n",
      "+----------+--------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_venue.\n",
      "+--------+--------------------+-------------------+-------------------+\n",
      "|venue_id|          venue_name|         date_start|           date_end|\n",
      "+--------+--------------------+-------------------+-------------------+\n",
      "|     AQC|     Aquatics Centre|2024-07-27 09:00:00|2024-08-10 20:00:00|\n",
      "|     AQC|     Aquatics Centre|2024-07-27 09:00:00|2024-08-10 20:00:00|\n",
      "|     AQC|     Aquatics Centre|2024-07-27 09:00:00|2024-08-10 20:00:00|\n",
      "|     BCY|         Bercy Arena|2024-07-27 09:00:00|2024-08-11 16:00:00|\n",
      "|     BCY|         Bercy Arena|2024-07-27 09:00:00|2024-08-11 16:00:00|\n",
      "|     BCY|         Bercy Arena|2024-07-27 09:00:00|2024-08-11 16:00:00|\n",
      "|     BOR|    Bordeaux Stadium|2024-07-25 17:00:00|2024-08-02 21:59:00|\n",
      "|     CDM| Champ de Mars Arena|2024-07-27 08:00:00|2024-08-11 12:00:00|\n",
      "|     CDM| Champ de Mars Arena|2024-07-27 08:00:00|2024-08-11 12:00:00|\n",
      "|     CTX|Chateauroux Shoot...|2024-07-27 07:00:00|2024-08-05 14:35:00|\n",
      "|     VER|Chteau de Versai...|2024-07-27 07:30:00|2024-08-11 11:30:00|\n",
      "|     VER|Chteau de Versai...|2024-07-27 07:30:00|2024-08-11 11:30:00|\n",
      "|     EIF|Eiffel Tower Stadium|2024-07-27 12:00:00|2024-08-10 21:59:00|\n",
      "|     ELA|      Elancourt Hill|2024-07-28 12:00:00|2024-07-29 14:30:00|\n",
      "|     STE|Geoffroy-Guichard...|2024-07-25 13:00:00|2024-07-31 19:00:00|\n",
      "|     LGN|       Golf National|2024-08-01 07:00:00|2024-08-10 16:30:00|\n",
      "|     GRP|        Grand Palais|2024-07-27 08:00:00|2024-08-10 21:00:00|\n",
      "|     GRP|        Grand Palais|2024-07-27 08:00:00|2024-08-10 21:00:00|\n",
      "|    null|      Htel de Ville|2024-08-10 06:00:00|2024-08-11 09:15:00|\n",
      "|     INV|           Invalides|2024-07-25 07:30:00|2024-08-04 13:20:00|\n",
      "+--------+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with get_sparkSession('check') as spark:\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/schedules_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df = df.withColumn('venue', regexp_replace(\"venue\", \"\\\\d\", \"\")) \\\n",
    "           .withColumn('venue', rtrim(col('venue'))) \\\n",
    "           .withColumn('venue_code', regexp_replace(\"venue_code\", \"\\\\d\", \"\")) \\\n",
    "           .withColumn('venue_code', rtrim(col('venue_code')))\n",
    "    df = df.select('venue_code', 'venue').distinct()\n",
    "    df = df.withColumnRenamed('venue', 'venue_schedule')\n",
    "    #df.show()\n",
    "    df2 = read_HDFS(spark, \"hdfs://namenode:9000/datalake/silver_storage/venues_silver\")\n",
    "    #df2.show()\n",
    "    df_join = df.join(df2, df['venue_schedule'] == df2['venue'], how = 'left')\n",
    "    df_join.show()\n",
    "    df3 = read_HDFS(spark, \"hdfs://namenode:9000/datalake/gold_storage/dim_venue\")\n",
    "    df3.show()\n",
    "    #df.filter(col('venue_code').isNull()).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede41b7-04b0-4e3e-a634-59ba3415a61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
