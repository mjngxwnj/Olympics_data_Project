{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb5ea0f",
   "metadata": {},
   "source": [
    "# 1. Demo Data Pipeline in Notebook\n",
    "This phase includes:\n",
    "- Hadoop - Spark Input and Output\n",
    "- Bronze\n",
    "- Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ee211",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f36570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, \n",
    "                               IntegerType, FloatType, ArrayType, \n",
    "                               DateType, ByteType, TimestampType)\n",
    "from pyspark.sql.functions import (col, split, regexp_replace, to_date,\n",
    "                                   monotonically_increasing_id, concat, \n",
    "                                   lit, substring, rtrim, ltrim, when, \n",
    "                                   length, explode, explode_outer, arrays_zip)\n",
    "from pyspark.sql import DataFrame\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43654663",
   "metadata": {},
   "source": [
    "## 1.1. Hadoop - Spark Input and Output\n",
    "I create functions for:\n",
    "- Creating SparkSession(default, snowflake configuration)\n",
    "- Handling Hadoop input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073581e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark Session\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local'):\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(appName)\n",
    "    conf.setMaster(master)\n",
    "    conf.set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.executor.cores\", \"2\")\n",
    "    \n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "    \n",
    "    print(f\"Successfully create SparkSession with app name: {appName}, master: {master}\\n\")\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        print(\"Spark Session has stopped!\")\n",
    "\n",
    "#input HDFS function\n",
    "def upload_HDFS(dataFrame: DataFrame, table_name: str, HDFS_path: str) -> None:\n",
    "    print(f'''Starting upload file \"{table_name}\" into {HDFS_path}...''')\n",
    "    #check types of parameters\n",
    "    if not isinstance(dataFrame, DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    if not isinstance(table_name, str):\n",
    "        raise TypeError(\"table name must be a string!\")\n",
    "    if not HDFS_path.startswith(\"hdfs://namenode:9000/\"):\n",
    "        raise TypeError('HDFS path must start with \"hdfs://namenode:9000/\"')\n",
    "    \n",
    "    #upload data\n",
    "    dataFrame.write.parquet(HDFS_path, mode = 'overwrite')\n",
    "    print(\"========================================================\")\n",
    "    print(f'''Successfully upload \"{table_name}\" into {HDFS_path}.''')\n",
    "    print(\"========================================================\")\n",
    "\n",
    "#read file from HDFS function\n",
    "def read_HDFS(spark: SparkSession, HDFS_path: str) -> DataFrame:\n",
    "    print(f\"Starting read file from {HDFS_path}.\")\n",
    "    #check parameters\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a Spark Session!\")\n",
    "    if not HDFS_path.startswith(\"hdfs://namenode:9000/\"):\n",
    "        raise TypeError('HDFS path must start with \"hdfs://namenode:9000/\"')\n",
    "    \n",
    "    #read file\n",
    "    data = spark.read.parquet(HDFS_path, header = True)\n",
    "    return data\n",
    "\n",
    "#set spark connection with snowflake data warehouse\n",
    "@contextmanager\n",
    "def get_snowflake_sparkSession(appName: str, master: str = 'local'):\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(appName)\n",
    "    conf.setMaster(master)\n",
    "    conf.set(\"spark.executor.memory\", \"2g\") \\\n",
    "        .set(\"spark.executor.cores\", \"2\") \\\n",
    "        .set(\"spark.jars\",\"/opt/jars/snowflake-jdbc-3.19.0.jar, \\\n",
    "                           /opt/jars/spark-snowflake_2.12-2.12.0-spark_3.4.jar\")\n",
    "    \n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "\n",
    "    print(f\"Successfully create SparkSession for Snowflake with app name: {appName}, master: {master}\\n\")\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        print(\"Spark Sesion has stopped!\")\n",
    "\n",
    "#default config for snowflake\n",
    "sfOptions_default = {\n",
    "    \"sfURL\": \"https://ae58556.ap-southeast-1.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"HUYNHTHUAN\",\n",
    "    \"sfPassword\": \"********\", #hide password\n",
    "    \"sfDatabase\": \"OLYMPICS_DB\",\n",
    "    \"sfSchema\": \"OLYMPICS_SCHEMA\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "#load data from hdfs to snowflake data warehouse\n",
    "def load_snowflake(dataFrame: DataFrame, table_name: str, sfOptions: dict = sfOptions_default):\n",
    "    print(f'''Starting upload {table_name} into snowflake...''')\n",
    "    #check parameters\n",
    "    if not isinstance(dataFrame, DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    if not isinstance(table_name, str):\n",
    "        raise TypeError(\"table name must be a string!\")\n",
    "    \n",
    "    #upload data\n",
    "    dataFrame.write \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**sfOptions) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(\"========================================================\")\n",
    "    print(f'''Successfully upload \"{table_name}\" into SnowFlake.''')\n",
    "    print(\"========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63bae59",
   "metadata": {},
   "source": [
    "## 1.2. Bronze\n",
    "In this phase, I will\n",
    "- Create schemas for nine tables. (1.2.1)\n",
    "- Apply the appropriate schema to each table and load all the data into HDFS as the Bronze layer. (1.2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb9929",
   "metadata": {},
   "source": [
    "### 1.2.1. Creating Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caae963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(table_name):\n",
    "    '''\n",
    "        Create schema for athletes table\n",
    "    '''\n",
    "    #list of columns containing string type\n",
    "    cols = ['code', 'name', 'name_short', 'name_tv',\n",
    "            'gender', 'function', 'country_code',\n",
    "            'country', 'country_full', 'nationality', \n",
    "            'nationality_full', 'nationality_code']\n",
    "    \n",
    "    cols2 = ['birth_place', 'birth_country', 'residence_place', \n",
    "             'residence_country', 'nickname', 'hobbies', 'occupation', \n",
    "             'education', 'family']\n",
    "    \n",
    "    cols3 = ['coach', 'reason', 'hero', 'influence', 'philosophy', \n",
    "             'sporting_relatives', 'ritual', 'other_sports']\n",
    "    \n",
    "    #create schema for athletes table\n",
    "    athletes_schema  = [StructField(col, StringType(), True) for col in cols]\n",
    "    athletes_schema += [StructField('height', IntegerType(), True),\n",
    "                        StructField('weight', IntegerType(), True),\n",
    "                        StructField('disciplines', ArrayType(StringType(),True), True),\n",
    "                        StructField('events', ArrayType(StringType(),True)),\n",
    "                        StructField('birth_date', DateType(), True)]\n",
    "    athletes_schema += [StructField(col, StringType(), True) for col in cols2]\n",
    "    athletes_schema += [StructField('lang', ArrayType(StringType(),True),True)]\n",
    "    athletes_schema += [StructField(col, StringType(), True) for col in cols3]\n",
    "    athletes_schema  =  StructType(athletes_schema)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Create schema for events table\n",
    "    '''\n",
    "    #events table has a suitable schema, so ignore it\n",
    "    events_schema = None\n",
    "    \n",
    "\n",
    "    '''\n",
    "        Create schema for medallists table \n",
    "    '''\n",
    "    medallists_schema = [StructField(\"medal_date\", DateType(), True),\n",
    "                         StructField(\"medal_type\", StringType(), True),\n",
    "                         StructField(\"medal_code\", ByteType(), True)]\n",
    "    \n",
    "    #list of columns containing string type\n",
    "    cols = ['name', 'gender', 'country', 'country_code', 'nationality','team', \n",
    "            'team_gender', 'discipline', 'event', 'event_type', 'url_event']\n",
    "    \n",
    "    medallists_schema += [StructField(col, StringType(), True) for col in cols]\n",
    "    medallists_schema += [StructField(\"birth_date\", DateType(), True),\n",
    "                          StructField(\"code\", StringType(), True)]\n",
    "    medallists_schema  =  StructType(medallists_schema)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Create schema for medals table\n",
    "    '''\n",
    "    medals_schema = [StructField(\"medal_type\", StringType(),True),\n",
    "                     StructField(\"medal_code\", ByteType(),True),\n",
    "                     StructField(\"medal_date\", DateType(), True)]\n",
    "    \n",
    "    #list of columns containing string type\n",
    "    cols = ['name', 'country_code', 'gender', 'discipline',\n",
    "             'event', 'event_type', 'url_event', 'code']\n",
    "    \n",
    "    medals_schema += [StructField(col, StringType(), True) for col in cols]\n",
    "    medals_schema  = StructType(medals_schema)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Create schema for schedules table\n",
    "    '''\n",
    "    schedules_schema = [StructField(\"start_date\", TimestampType(), True),\n",
    "                        StructField(\"end_date\", TimestampType(), True),\n",
    "                        StructField(\"day\", DateType(), True),\n",
    "                        StructField(\"status\", StringType(), True),\n",
    "                        StructField(\"discipline\", StringType(), True),\n",
    "                        StructField(\"discipline_code\", StringType(), True),\n",
    "                        StructField(\"event\", StringType(), True),\n",
    "                        StructField(\"event_medal\", IntegerType(), True)]\n",
    "    \n",
    "    #list of columns containing string type \n",
    "    cols = ['phase', 'gender', 'event_type', 'venue', \n",
    "            'venue_code', 'location_description', 'location_code']\n",
    "    \n",
    "    schedules_schema += [StructField(col, StringType(), False) for col in cols]\n",
    "    schedules_schema += [StructField(\"url\", StringType(), True)]\n",
    "    schedules_schema  = StructType(schedules_schema)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Create schema for schedules_preliminary schemas\n",
    "\n",
    "    '''\n",
    "    schedules_pre_schema = [StructField(\"date_start_utc\", TimestampType(), True),\n",
    "                            StructField(\"date_end_utc\", TimestampType(), True),\n",
    "                            StructField(\"estimated\", StringType(), True),\n",
    "                            StructField(\"estimated_start\", StringType(), True),\n",
    "                            StructField(\"start_text\", StringType(), True),\n",
    "                            StructField(\"medal\", IntegerType(), True),\n",
    "                            StructField(\"venue_code\", StringType(), True),\n",
    "                            StructField(\"description\", StringType(), True)]\n",
    "    \n",
    "    #list of columns containing string type - can be nullable\n",
    "    cols = ['venue_code_other', 'discription_other', \n",
    "            'team_1_code', 'team_1', 'team_2_code', 'team_2']\n",
    "    \n",
    "    schedules_pre_schema += [StructField(col, StringType(), True) for col in cols]\n",
    "    schedules_pre_schema += [StructField(\"tag\", StringType(), True),\n",
    "                             StructField(\"sport\", StringType(), True),\n",
    "                             StructField(\"sport_code\", StringType(), True),\n",
    "                             StructField(\"sport_url\", StringType(), True)]\n",
    "    schedules_pre_schema  = StructType(schedules_pre_schema)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Create schema for teams table\n",
    "    '''\n",
    "    #list of column containing string type\n",
    "    cols = ['code', 'team', 'team_gender', 'country', 'country_full', \n",
    "            'country_code', 'discipline', 'disciplines_code', 'events']\n",
    "    \n",
    "    teams_schema  = [StructField(col, StringType(), True) for col in cols]\n",
    "    teams_schema += [StructField(\"athletes\", ArrayType(StringType(),True)),\n",
    "                     StructField(\"coaches\", ArrayType(StringType(),True)),\n",
    "                     StructField(\"athletes_codes\",ArrayType(StringType(),True)),\n",
    "                     StructField(\"num_athletes\",IntegerType(),True),\n",
    "                     StructField(\"coaches_codes\",ArrayType(StringType(),True)),\n",
    "                     StructField(\"num_coaches\",IntegerType(),True)]\n",
    "    teams_schema = StructType(teams_schema)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Create schema for torch_route table\n",
    "    '''\n",
    "    torch_route_schema = [StructField(\"title\", StringType(), True),\n",
    "                          StructField(\"city\", StringType(), True),\n",
    "                          StructField(\"date_start\", TimestampType(), True),\n",
    "                          StructField(\"date_end\", TimestampType(), True),\n",
    "                          StructField(\"tag\", StringType(), True),\n",
    "                          StructField(\"url\", StringType(), True),\n",
    "                          StructField(\"stage_number\", IntegerType(), True)]\n",
    "    torch_route_schema = StructType(torch_route_schema)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Create schema for venues table\n",
    "    '''\n",
    "    venues_schema = [StructField(\"venue\", StringType(), True),\n",
    "                     StructField(\"sports\", ArrayType(StringType(),True),True),\n",
    "                     StructField(\"date_start\", TimestampType(), True),\n",
    "                     StructField(\"date_end\", TimestampType(), True),\n",
    "                     StructField(\"tag\", StringType(), True),\n",
    "                     StructField(\"url\", StringType(), True)]\n",
    "    venues_schema = StructType(venues_schema)\n",
    "\n",
    "\n",
    "    #create dict for mapping schema\n",
    "    schema = {\n",
    "        'athletes' : athletes_schema,\n",
    "        'events' : events_schema,\n",
    "        'medallists' : medallists_schema,\n",
    "        'medals' : medals_schema,\n",
    "        'schedules_preliminary' : schedules_pre_schema,\n",
    "        'schedules' : schedules_schema,\n",
    "        'teams' : teams_schema,\n",
    "        'torch_route' : torch_route_schema,\n",
    "        'venues' : venues_schema\n",
    "    }\n",
    "\n",
    "    return schema[table_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15144d1c",
   "metadata": {},
   "source": [
    "### 1.2.2. Applying Schemas and Loading Data into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff91fda-a51a-4026-84ec-8a15782aec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Bronze task starts!========================\n",
      "Successfully create SparkSession with app name: bronze_task_spark, master: local\n",
      "\n",
      "Starting upload file \"athletes\" into hdfs://namenode:9000/datalake/bronze_storage//athletes/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"athletes\" into hdfs://namenode:9000/datalake/bronze_storage//athletes/.\n",
      "========================================================\n",
      "Starting upload file \"events\" into hdfs://namenode:9000/datalake/bronze_storage//events/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"events\" into hdfs://namenode:9000/datalake/bronze_storage//events/.\n",
      "========================================================\n",
      "Starting upload file \"medallists\" into hdfs://namenode:9000/datalake/bronze_storage//medallists/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"medallists\" into hdfs://namenode:9000/datalake/bronze_storage//medallists/.\n",
      "========================================================\n",
      "Starting upload file \"medals\" into hdfs://namenode:9000/datalake/bronze_storage//medals/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"medals\" into hdfs://namenode:9000/datalake/bronze_storage//medals/.\n",
      "========================================================\n",
      "Starting upload file \"schedules_preliminary\" into hdfs://namenode:9000/datalake/bronze_storage//schedules_preliminary/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"schedules_preliminary\" into hdfs://namenode:9000/datalake/bronze_storage//schedules_preliminary/.\n",
      "========================================================\n",
      "Starting upload file \"schedules\" into hdfs://namenode:9000/datalake/bronze_storage//schedules/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"schedules\" into hdfs://namenode:9000/datalake/bronze_storage//schedules/.\n",
      "========================================================\n",
      "Starting upload file \"teams\" into hdfs://namenode:9000/datalake/bronze_storage//teams/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"teams\" into hdfs://namenode:9000/datalake/bronze_storage//teams/.\n",
      "========================================================\n",
      "Starting upload file \"torch_route\" into hdfs://namenode:9000/datalake/bronze_storage//torch_route/...\n",
      "========================================================\n",
      "Successfully upload \"torch_route\" into hdfs://namenode:9000/datalake/bronze_storage//torch_route/.\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload file \"venues\" into hdfs://namenode:9000/datalake/bronze_storage//venues/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"venues\" into hdfs://namenode:9000/datalake/bronze_storage//venues/.\n",
      "========================================================\n",
      "Spark Sesion has stopped!\n",
      "========================Bronze task finishes!========================\n"
     ]
    }
   ],
   "source": [
    "#task\n",
    "def bronze_task(tables: list, HDFS_path: str):\n",
    "    with get_sparkSession(\"bronze_task_spark\", \"local\") as spark:\n",
    "        df = None\n",
    "        for table_name in tables:\n",
    "            '''\n",
    "                Note that we need to preprocess data for athletes,\n",
    "                teams and venues table before applying the schema\n",
    "            '''\n",
    "            if table_name == 'athletes':\n",
    "                #read data from csv file\n",
    "                data = spark.read.csv(\"/opt/data/athletes.csv\", header = True)\n",
    "\n",
    "                #replace unnecessary character \n",
    "                data = data.withColumn(\"disciplines\", regexp_replace(\"disciplines\", \"[\\[\\]']\", \"\")) \\\n",
    "                           .withColumn(\"events\", regexp_replace(\"events\",\"[\\[\\]']\",\"\")) \n",
    "                \n",
    "                data = data.withColumn(\"disciplines\", split(data[\"disciplines\"],\",\")) \\\n",
    "                           .withColumn(\"events\", split(data[\"events\"],\",\")) \\\n",
    "                           .withColumn(\"height\", col(\"height\").cast(\"int\")) \\\n",
    "                           .withColumn(\"weight\", col(\"weight\").cast(\"int\")) \\\n",
    "                           .withColumn(\"birth_date\", to_date(col(\"birth_date\"), \"yyyy-mm-dd\")) \\\n",
    "                           .withColumn(\"lang\", split(data[\"lang\"],\",\"))\n",
    "                \n",
    "                #create dataFrame\n",
    "                df = spark.createDataFrame(data.rdd, schema = get_schema(table_name))\n",
    "\n",
    "\n",
    "            elif table_name == \"teams\":\n",
    "                #read data from csv file\n",
    "                data = spark.read.csv(\"/opt/data/teams.csv\", header = True)\n",
    "\n",
    "                #replace unnecessary characters\n",
    "                data = data.withColumn(\"athletes\", regexp_replace(\"athletes\",\"[\\[\\]']\",\"\")) \\\n",
    "                           .withColumn(\"coaches\", regexp_replace(\"coaches\",\"[\\[\\]']\",\"\")) \\\n",
    "                           .withColumn(\"athletes_codes\", regexp_replace(\"athletes_codes\",\"[\\[\\]']\",\"\")) \\\n",
    "                           .withColumn(\"coaches_codes\", regexp_replace(\"coaches_codes\",\"[\\[\\]']\",\"\"))\n",
    "                \n",
    "                #transform data type\n",
    "                data = data.withColumn(\"athletes\", split(data[\"athletes\"],\",\")) \\\n",
    "                           .withColumn(\"coaches\", split(data[\"coaches\"],\",\")) \\\n",
    "                           .withColumn(\"athletes_codes\", split(data[\"athletes_codes\"],\",\")) \\\n",
    "                           .withColumn(\"coaches_codes\", split(data[\"coaches_codes\"],\",\")) \\\n",
    "                           .withColumn(\"num_athletes\", col(\"num_athletes\").cast(\"int\")) \\\n",
    "                           .withColumn(\"num_coaches\", col(\"num_coaches\").cast(\"int\"))\n",
    "                 \n",
    "                #create dataFrame\n",
    "                df = spark.createDataFrame(data.rdd, schema = get_schema(table_name))\n",
    "\n",
    "\n",
    "            elif table_name == 'venues':\n",
    "                #read data from csv file\n",
    "                data = spark.read.csv(\"/opt/data/venues.csv\", header = True)\n",
    "\n",
    "                #replace unnecessary characters\n",
    "                data = data.withColumn(\"sports\", regexp_replace(\"sports\",\"[\\[\\]']\",\"\"))\n",
    "\n",
    "                data = data.withColumn(\"sports\", split(data[\"sports\"],\",\")) \\\n",
    "                           .withColumn(\"date_start\", col(\"date_start\").cast(\"timestamp\")) \\\n",
    "                           .withColumn(\"date_end\", col(\"date_end\").cast(\"timestamp\"))\n",
    "                \n",
    "                #create dataFrame\n",
    "                df = spark.createDataFrame(data.rdd, schema = get_schema(table_name)) \n",
    "\n",
    "\n",
    "            else:\n",
    "                df = spark.read.csv(f\"/opt/data/{table_name}.csv\",header = True, schema = get_schema(table_name))\n",
    "\n",
    "\n",
    "            #After reading csv files or preprocessing, we load data into HDFS\n",
    "            upload_HDFS(df, table_name, HDFS_path + f'/{table_name}/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #List all tables\n",
    "    tables = ['athletes', 'events', 'medallists', \n",
    "            'medals', 'schedules_preliminary', \n",
    "            'schedules', 'teams', 'torch_route', 'venues']\n",
    "    \n",
    "    #HDFS path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/\"\n",
    "    \n",
    "    print(\"=========================Bronze task starts!========================\")\n",
    "\n",
    "    bronze_task(tables, HDFS_path)\n",
    "    \n",
    "    print(\"========================Bronze task finishes!========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa212b",
   "metadata": {},
   "source": [
    "## 1.3 Silver\n",
    "In this phase, I will handle each table individually, including:\n",
    "- Creating a SilverLayer class to process and transform data, which has six main functions (1.3.1):\n",
    "    - Drop N/A columns\n",
    "    - Drop duplicates\n",
    "    - Drop unnecessary columns\n",
    "    - Rename columns\n",
    "    - Handle nested structures\n",
    "    - Handle missing values\n",
    "- Using the SilverLayer class to process all tables. (1.3.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a6a890",
   "metadata": {},
   "source": [
    "### 1.3.1. Creating SilverLayer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fc931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create silver clean classs\n",
    "class Silverlayer:\n",
    "    #init\n",
    "    def __init__(self, df: DataFrame, \n",
    "                 dropna_columns: list = None,\n",
    "                 columns_dropDuplicates: list = None,\n",
    "                 columns_drop: list = None, \n",
    "                 columns_rename: dict = None, \n",
    "                 nested_columns: list = None,\n",
    "                 missval_columns: dict = None,\n",
    "                 ):\n",
    "        \n",
    "        '''\n",
    "            Initializes processing task\n",
    "        '''\n",
    "        #firstly, check all types of parameters \n",
    "        if df is not None and not isinstance(df, DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame\")\n",
    "        if columns_dropDuplicates is not None and not isinstance(columns_dropDuplicates, list):\n",
    "            raise TypeError(\"columns_dropDuplicates must be a list\")\n",
    "        if columns_drop is not None and not isinstance(columns_drop, list):\n",
    "            raise TypeError(\"columns_drop must be a list\")\n",
    "        if columns_rename is not None and not isinstance(columns_rename, dict):\n",
    "            raise TypeError(\"columns_rename must be a dict\")\n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"nested_columns must be a list\")\n",
    "        if missval_columns is not None and not isinstance(missval_columns, dict):\n",
    "            raise TypeError(\"columns_null must be a dict\")\n",
    "        if dropna_columns is not None and not isinstance(dropna_columns, list):\n",
    "            raise TypeError(\"dropna_columns must be a list\")\n",
    "        \n",
    "        #data frame\n",
    "        self.df = df\n",
    "\n",
    "        #list of columns to apply the drop duplicate function\n",
    "        self.columns_dropDuplicates = columns_dropDuplicates\n",
    "\n",
    "        #list of columns to drop\n",
    "        self.columns_drop = columns_drop\n",
    "\n",
    "        #dict containing old name & new name\n",
    "        self.columns_rename = columns_rename\n",
    "\n",
    "        #list of columns that need to handle nested structures\n",
    "        self.nested_columns = nested_columns\n",
    "\n",
    "        #dict containing columns to check & a value to apply for nulls\n",
    "        self.missval_columns = missval_columns \n",
    "\n",
    "        #list of columns to apply drop na function\n",
    "        self.dropna_columns = dropna_columns\n",
    "\n",
    "    def drop_na(self, df: DataFrame, dropna_columns: list):\n",
    "        '''\n",
    "            Drop rows containing na values\n",
    "        '''\n",
    "        self.df = df.dropna(how = 'all', subset = dropna_columns)\n",
    "\n",
    "    def drop_duplicates(self, df: DataFrame, columns_dropDuplicates: list):\n",
    "        '''\n",
    "            Drop duplicates based on specified columns\n",
    "        '''\n",
    "        self.df = df.dropDuplicates(columns_dropDuplicates)\n",
    "\n",
    "    def drop_columns(self, df: DataFrame, columns_drop: list):\n",
    "        '''\n",
    "            Drop unnecessary columns\n",
    "        '''\n",
    "        self.df = df.drop(*columns_drop)\n",
    "\n",
    "    def rename_columns(self, columns_rename: dict):\n",
    "        '''\n",
    "            Rename columns\n",
    "        '''\n",
    "        for old_name, new_name in columns_rename.items():\n",
    "            self.df = self.df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    def handle_nested(self, nested_columns: list):\n",
    "        '''\n",
    "            Handle nested columns \n",
    "        '''\n",
    "        for col in nested_columns:\n",
    "            self.df = self.df.withColumn(col, explode_outer(col))\n",
    "            self.df = self.df.withColumn(col, ltrim(col))\n",
    "\n",
    "    def handle_missing(self, missval_columns: dict):\n",
    "        '''\n",
    "            Handle missing values\n",
    "        '''\n",
    "        for col, value in missval_columns.items():\n",
    "            self.df = self.df.fillna(value = value, subset = col)\n",
    "\n",
    "    def process(self) -> DataFrame:\n",
    "        '''\n",
    "            Process based on all parameters\n",
    "        '''\n",
    "        self.drop_na(self.df, self.dropna_columns)\n",
    "        self.drop_duplicates(self.df, self.columns_dropDuplicates)\n",
    "\n",
    "        if self.columns_drop:\n",
    "            self.drop_columns(self.df, self.columns_drop)\n",
    "        \n",
    "        if self.columns_rename:\n",
    "            self.rename_columns(self.columns_rename)\n",
    "\n",
    "        if self.nested_columns:\n",
    "            self.handle_nested(self.nested_columns)\n",
    "        \n",
    "        if self.missval_columns:\n",
    "            self.handle_missing(self.missval_columns)\n",
    "\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928722fb",
   "metadata": {},
   "source": [
    "### 1.3.2 Applying SilverLayer class to all tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b04bede",
   "metadata": {},
   "source": [
    "#### Athletes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0646d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#athletes\n",
    "def athletes_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process athletes table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/athletes\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    columns_drop = ['name_short', 'name_tv', 'hobbies', 'occupation', 'education', \n",
    "                    'family', 'coach', 'reason', 'hero', 'influence', 'philosophy', \n",
    "                    'sporting_relatives', 'ritual', 'other_sports']\n",
    "    \n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            columns_drop    =  columns_drop,\n",
    "                            columns_rename  = {'code':'athletes_id', 'name':'full_name', 'lang':'language'},\n",
    "                            nested_columns  = ['disciplines', 'events', 'language'],\n",
    "                            missval_columns = {'birth_place':'N/A', \n",
    "                                                'birth_country':'N/A', \n",
    "                                                'nickname': 'N/A',\n",
    "                                                'residence_place':'N/A', \n",
    "                                                'residence_country':'N/A',\n",
    "                                                'height':0, 'weight':0}).process()\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, \"athletes_silver\", HDFS_load + '/athletes_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1ce76",
   "metadata": {},
   "source": [
    "#### Events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54878863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#events\n",
    "def events_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process events table\n",
    "    '''\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/events\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            columns_dropDuplicates = ['event', 'sport'],\n",
    "                            columns_drop           = ['sport_url', 'tag'], \n",
    "                            columns_rename         = {'sport_code':'id_sport'}).process()\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'events_silver', HDFS_load + '/events_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b727f",
   "metadata": {},
   "source": [
    "#### Medallists table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60fcf23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#medallists\n",
    "def medallists_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process medallists table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/medallists\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            dropna_columns         = ['name'], \n",
    "                            columns_dropDuplicates = ['name', 'medal_type', 'discipline', 'event'],\n",
    "                            columns_drop           = ['medal_code', 'url_event'],\n",
    "                            columns_rename         = {'name':'full_name', 'code':'athletes_id'},\n",
    "                            missval_columns        = {'team':'N/A','team_gender':'N/A', 'nationality':'N/A'}).process()\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'medallists_silver', HDFS_load + '/medallists_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6f66c",
   "metadata": {},
   "source": [
    "#### Medals table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da6025b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#medals\n",
    "def medals_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process medals table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/medals\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['name'], \n",
    "                            columns_dropDuplicates = ['name', 'medal_type', 'discipline', 'event'],\n",
    "                            columns_drop           = ['medal_code', 'url_event'],\n",
    "                            columns_rename         = {'name':'full_name', 'code':'athletes_id'}).process()\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'medals_silver', HDFS_load + '/medals_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce68673",
   "metadata": {},
   "source": [
    "#### Schedules Table\n",
    "Due to a mismatch with data in the venues table, I will transform the data in the venue column of the schedules table after applying the SilverLayer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87c5a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schedules\n",
    "def schedules_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process schedules table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/schedules\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['event'],\n",
    "                            columns_dropDuplicates = ['event', 'discipline', 'phase'],\n",
    "                            columns_drop           = ['status', 'event_medal', 'url']).process()\n",
    "    \n",
    "    #preprocess to match name of venue before joining \n",
    "    df_silver = df_silver.withColumn('venue', regexp_replace(\"venue\", \"\\\\d\", \"\")) \\\n",
    "                         .withColumn('venue', rtrim('venue')) \\\n",
    "                         .withColumn('venue_code', regexp_replace(\"venue_code\", \"\\\\d\", \"\")) \\\n",
    "                         .withColumn('venue_code', rtrim('venue_code'))\n",
    "    \n",
    "    #process data to match venue\n",
    "    df_silver = df_silver.withColumn('venue', when(col('venue') == 'Chateauroux Shooting Ctr', 'Chateauroux Shooting Centre') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'Nautical St - Flat water', 'Vaires-sur-Marne Nautical Stadium') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'BMX Stadium', 'Saint-Quentin-en-Yvelines BMX Stadium') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'Champ-de-Mars Arena', 'Champ de Mars Arena') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'Le Bourget Climbing Venue', 'Le Bourget Sport Climbing Venue') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'Nautical St - White water', 'Vaires-sur-Marne Nautical Stadium') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'Roland-Garros Stadium', 'Stade Roland-Garros') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'Le Golf National', 'Golf National') \\\n",
    "                         .otherwise(col('venue'))) \\\n",
    "                         .withColumn('venue', when(col('venue') == 'National Velodrome', 'Saint-Quentin-en-Yvelines Velodrome') \\\n",
    "                         .otherwise(col('venue')))\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'schedules_silver', HDFS_load + '/schedules_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47c4fc",
   "metadata": {},
   "source": [
    "#### Schedules preliminary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ef6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schedules_preliminary\n",
    "def schedules_pre_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process schedules preliminary table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/schedules_preliminary\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['team_1_code', 'team_2_code', 'team_1', 'team_2'],\n",
    "                            columns_dropDuplicates = ['team_1_code', 'team_2_code', 'sport'],\n",
    "                            columns_drop           = ['estimated', 'estimated_start', 'start_text', \n",
    "                                                    'medal', 'sport_url', 'tag'],\n",
    "                            missval_columns        = {'venue_code':'N/A', 'venue_code_other':'N/A',\n",
    "                                                    'discription_other':'N/A'}).process()\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'schedules_pre_silver', HDFS_load + '/schedules_pre_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adfc59b",
   "metadata": {},
   "source": [
    "#### Teams Table\n",
    "To handle the nested structure in this table, we need to merge the athletes and athletes_code arrays, and then explode them to match the two related arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dc09882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#teams\n",
    "def teams_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process teams table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/teams\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    #first, we need to merge array athletes and athletes_code to handle nested structure\n",
    "    df = df.withColumn('athletes_id_merge', arrays_zip('athletes','athletes_codes'))\n",
    "    \n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['code', 'team'],\n",
    "                            columns_drop           = ['coaches', 'coaches_codes', 'num_coaches', \n",
    "                                                      'athletes', 'athletes_codes'],\n",
    "                            columns_rename         = {'events':'event', 'code':'team_id', 'team':'team_name'},\n",
    "                            missval_columns        = {'event':'Default'}).process()\n",
    "    \n",
    "    #After silver processing, we process nested structure\n",
    "    df_silver = df_silver.withColumn('athletes_id_merge', explode('athletes_id_merge'))\n",
    "\n",
    "    df_silver = df_silver.withColumn('athletes', col('athletes_id_merge.athletes')) \\\n",
    "                         .withColumn('athletes_id', col('athletes_id_merge.athletes_codes'))\n",
    "    \n",
    "    df_silver = df_silver.withColumn('athletes', ltrim('athletes')) \\\n",
    "                         .withColumn('athletes_id', ltrim('athletes_id'))\n",
    "    \n",
    "    df_silver = df_silver.drop('athletes_id_merge')\n",
    "\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'teams_silver', HDFS_load + '/teams_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88755c6a",
   "metadata": {},
   "source": [
    "#### Torch route table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff5f334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_route\n",
    "def torch_route_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process torch route table\n",
    "    '''\n",
    "    #read filey\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/torch_route\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df, \n",
    "                            dropna_columns         = ['title'],\n",
    "                            columns_dropDuplicates = ['title'],\n",
    "                            columns_drop           = ['tag', 'url'],\n",
    "                            missval_columns        = {'city':'N/A', 'stage_number':0}).process()\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'torch_route_silver', HDFS_load + '/torch_route_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ebb74",
   "metadata": {},
   "source": [
    "#### Venues table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c8e46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#venues\n",
    "def venues_silver(spark: SparkSession, HDFS_load):\n",
    "    '''\n",
    "        Process venues table\n",
    "    '''\n",
    "    #read file\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/bronze_storage/venues\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "\n",
    "    #process\n",
    "    df_silver = Silverlayer(df = df,\n",
    "                            dropna_columns         = ['venue'],\n",
    "                            columns_dropDuplicates = ['venue'],\n",
    "                            columns_drop           = ['tag', 'url'],\n",
    "                            columns_rename         = {'sports':'sport'},\n",
    "                            nested_columns         = ['sport'],\n",
    "                            missval_columns        = {'date_start':'N/A', 'date_end':'N/A'}).process()\n",
    "    \n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_silver, 'venues_silver', HDFS_load + '/venues_silver/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce019314",
   "metadata": {},
   "source": [
    "#### Running all functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a4bae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Silver task starts!========================\n",
      "Successfully create SparkSession with app name: silver_task_spark, master: local\n",
      "\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/athletes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload file \"athletes_silver\" into hdfs://namenode:9000/datalake/silver_storage/athletes_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"athletes_silver\" into hdfs://namenode:9000/datalake/silver_storage/athletes_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/events.\n",
      "Starting upload file \"events_silver\" into hdfs://namenode:9000/datalake/silver_storage/events_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"events_silver\" into hdfs://namenode:9000/datalake/silver_storage/events_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/medallists.\n",
      "Starting upload file \"medallists_silver\" into hdfs://namenode:9000/datalake/silver_storage/medallists_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"medallists_silver\" into hdfs://namenode:9000/datalake/silver_storage/medallists_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/medals.\n",
      "Starting upload file \"medals_silver\" into hdfs://namenode:9000/datalake/silver_storage/medals_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"medals_silver\" into hdfs://namenode:9000/datalake/silver_storage/medals_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/schedules_preliminary.\n",
      "Starting upload file \"schedules_pre_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_pre_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"schedules_pre_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_pre_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/schedules.\n",
      "Starting upload file \"schedules_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"schedules_silver\" into hdfs://namenode:9000/datalake/silver_storage/schedules_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/teams.\n",
      "Starting upload file \"teams_silver\" into hdfs://namenode:9000/datalake/silver_storage/teams_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"teams_silver\" into hdfs://namenode:9000/datalake/silver_storage/teams_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/torch_route.\n",
      "Starting upload file \"torch_route_silver\" into hdfs://namenode:9000/datalake/silver_storage/torch_route_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"torch_route_silver\" into hdfs://namenode:9000/datalake/silver_storage/torch_route_silver/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/bronze_storage/venues.\n",
      "Starting upload file \"venues_silver\" into hdfs://namenode:9000/datalake/silver_storage/venues_silver/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"venues_silver\" into hdfs://namenode:9000/datalake/silver_storage/venues_silver/.\n",
      "========================================================\n",
      "Spark Sesion has stopped!\n",
      "========================Silver task finishes!========================\n"
     ]
    }
   ],
   "source": [
    "def silver_task(spark, HDFS_load):\n",
    "    athletes_silver(spark, HDFS_load)\n",
    "    events_silver(spark, HDFS_load)\n",
    "    medallists_silver(spark, HDFS_load)\n",
    "    medals_silver(spark, HDFS_load)\n",
    "    schedules_pre_silver(spark, HDFS_load)\n",
    "    schedules_silver(spark, HDFS_load)\n",
    "    teams_silver(spark, HDFS_load)\n",
    "    torch_route_silver(spark, HDFS_load)\n",
    "    venues_silver(spark, HDFS_load)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #hdfs path \n",
    "    HDFS_load = \"hdfs://namenode:9000/datalake/silver_storage\"\n",
    "\n",
    "    print(\"=========================Silver task starts!========================\")\n",
    "\n",
    "    with get_sparkSession(\"silver_task_spark\") as spark:\n",
    "        silver_task(spark, HDFS_load)\n",
    "        \n",
    "    print(\"========================Silver task finishes!========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8af49611-1b1f-46b9-9e03-6e289ce0ecb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m HDFS_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:9000/datalake/silver_storage/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#call \u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msparkSession\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_data_quality_spark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m spark:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[1;32m     31\u001b[0m         check_data(spark, HDFS_path, table)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Check data quality of tables\n",
    "'''\n",
    "def check_data(spark: SparkSession, HDFS_path: str, table_name: str):\n",
    "    '''\n",
    "        Check data of all tables\n",
    "    '''\n",
    "    #read file\n",
    "    df = read_HDFS(spark, HDFS_path + f'/{table_name}/')\n",
    "    print(f'Checking data for table \"{table_name}\"...')\n",
    "    df.show(5, truncate = False)\n",
    "\n",
    "    #count the number of null values in each column of the table\n",
    "    df_checkNull = {col:df.filter(df[col].isNull()).count() for col in df.columns}\n",
    "    for col, num in df_checkNull.items():\n",
    "        print(f'Number of null values in column \"{col}\": {num}')\n",
    "\n",
    "    #print schema to ensure that all data doesn't have nested structure(array type of struct type)\n",
    "    df.printSchema()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #list all tables\n",
    "    tables = ['athletes_silver', 'events_silver', 'medallists_silver', \n",
    "              'medals_silver', 'schedules_preliminary_silver', 'schedules_silver', \n",
    "              'teams_silver', 'torch_route_silver', 'venues_silver']\n",
    "    #path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/\"\n",
    "    #call \n",
    "    with sparkSession(\"check_data_quality_spark\", \"local\") as spark:\n",
    "        for table in tables:\n",
    "            check_data(spark, HDFS_path, table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a24ea69-d23f-4fea-b3f5-4811ecf8556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Gold task starts!========================\n",
      "Successfully create SparkSession with app name: gold_task_spark, master: local\n",
      "\n",
      "Starting upload file \"dim_medal\" into hdfs://namenode:9000/datalake/gold_storage/dim_medal/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"dim_medal\" into hdfs://namenode:9000/datalake/gold_storage/dim_medal/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/schedules_silver.\n",
      "Starting upload file \"dim_discipline\" into hdfs://namenode:9000/datalake/gold_storage/dim_discipline/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"dim_discipline\" into hdfs://namenode:9000/datalake/gold_storage/dim_discipline/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/events_silver.\n",
      "Starting upload file \"dim_event\" into hdfs://namenode:9000/datalake/gold_storage/dim_event/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"dim_event\" into hdfs://namenode:9000/datalake/gold_storage/dim_event/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/athletes_silver.\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/teams_silver.\n",
      "Starting upload file \"dim_country\" into hdfs://namenode:9000/datalake/gold_storage/dim_country/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"dim_country\" into hdfs://namenode:9000/datalake/gold_storage/dim_country/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/medals_silver.\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_discipline.\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_event.\n",
      "Starting upload file \"fact_medallist\" into hdfs://namenode:9000/datalake/gold_storage/fact_medallist/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"fact_medallist\" into hdfs://namenode:9000/datalake/gold_storage/fact_medallist/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/athletes_silver.\n",
      "Starting upload file \"dim_athletes\" into hdfs://namenode:9000/datalake/gold_storage/dim_athletes/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"dim_athletes\" into hdfs://namenode:9000/datalake/gold_storage/dim_athletes/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/medals_silver.\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_discipline.\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_event.\n",
      "Starting upload file \"fact_medal_team\" into hdfs://namenode:9000/datalake/gold_storage/fact_medal_team/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"fact_medal_team\" into hdfs://namenode:9000/datalake/gold_storage/fact_medal_team/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/schedules_silver.\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_discipline.\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_event.\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/venues_silver.\n",
      "Starting upload file \"fact_schedule\" into hdfs://namenode:9000/datalake/gold_storage/fact_schedule/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Successfully upload \"fact_schedule\" into hdfs://namenode:9000/datalake/gold_storage/fact_schedule/.\n",
      "========================================================\n",
      "Starting upload file \"dim_venue\" into hdfs://namenode:9000/datalake/gold_storage/dim_venue/...\n",
      "========================================================\n",
      "Successfully upload \"dim_venue\" into hdfs://namenode:9000/datalake/gold_storage/dim_venue/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/teams_silver.\n",
      "Starting upload file \"dim_team\" into hdfs://namenode:9000/datalake/gold_storage/dim_team/...\n",
      "========================================================\n",
      "Successfully upload \"dim_team\" into hdfs://namenode:9000/datalake/gold_storage/dim_team/.\n",
      "========================================================\n",
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/teams_silver.\n",
      "Starting upload file \"dim_athletes_team\" into hdfs://namenode:9000/datalake/gold_storage/dim_athletes_team/...\n",
      "========================================================\n",
      "Successfully upload \"dim_athletes_team\" into hdfs://namenode:9000/datalake/gold_storage/dim_athletes_team/.\n",
      "========================================================\n",
      "========================Gold task finishes!========================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (monotonically_increasing_id, concat, \n",
    "                                   lit, substring, rtrim, when, length)\n",
    "\n",
    "#dim table - medal \n",
    "def dim_medal(spark: SparkSession, HDFS_load):\n",
    "    data = [(1,'Gold Medal'),(2,'Silver Medal'),(3,'Bronze Medal')]\n",
    "    df_gold = spark.createDataFrame(data, schema = \"medal_id int, medal_type string\")\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_medal', HDFS_load + '/dim_medal/')\n",
    "\n",
    "#dim table - discipline\n",
    "def dim_discipline(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/schedules_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    #select\n",
    "    df_gold = df.select('discipline_code', 'discipline').distinct() \\\n",
    "                .withColumnRenamed('discipline_code', 'discipline_id') \\\n",
    "                .withColumnRenamed('discipline', 'discipline_type')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_discipline', HDFS_load + '/dim_discipline/')\n",
    "\n",
    "#dim table - event\n",
    "def dim_event(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/events_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df = df.select('event').distinct()\n",
    "    #add column\n",
    "    df = df.withColumn('event_id', monotonically_increasing_id())\n",
    "    df = df.withColumn('event_id', concat(lit('ev'), col('event_id')))\n",
    "    #rename\n",
    "    df_gold = df.withColumnRenamed('event', 'event_type')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_event', HDFS_load + '/dim_event/')\n",
    "    \n",
    "#dim table - country\n",
    "def dim_country(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_athletes_path = \"hdfs://namenode:9000/datalake/silver_storage/athletes_silver\"\n",
    "    HDFS_team_path = \"hdfs://namenode:9000/datalake/silver_storage/teams_silver\"\n",
    "    df1 = read_HDFS(spark, HDFS_athletes_path)\n",
    "    df2 = read_HDFS(spark, HDFS_team_path)\n",
    "    #select\n",
    "    df_nationality = df1.select('country_code', 'country')\n",
    "    df_country = df1.select('nationality_code', 'nationality')\n",
    "    df_team_country = df2.select('country_code', 'country')\n",
    "    #union column\n",
    "    df_gold = df_nationality.union(df_country).distinct()\n",
    "    df_gold = df_gold.union(df_team_country).distinct()\n",
    "    #rename\n",
    "    df_gold = df_gold.withColumnRenamed('country_code', 'country_id') \\\n",
    "                     .withColumnRenamed('country', 'country_name')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_country', HDFS_load + '/dim_country/')\n",
    "    \n",
    "#fact table - medallist\n",
    "def fact_medallist(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/medals_silver\"\n",
    "    HDFS_discipline_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_discipline\"\n",
    "    HDFS_event_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_event\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df_discipline = read_HDFS(spark, HDFS_discipline_gold)\n",
    "    df_event = read_HDFS(spark, HDFS_event_gold)\n",
    "    \n",
    "    df = df.filter(length('athletes_id') == 7)\n",
    "    df = df.select('athletes_id', 'medal_type', 'medal_date', 'discipline', 'event')\n",
    "    #handle table individually\n",
    "    df = df.withColumn('medallist_id', concat(col('athletes_id'),substring('medal_type',1,1))) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Bronze Medal', '3')) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Silver Medal', '2')) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Gold Medal', '1')) \\\n",
    "           .withColumn('medal_type', col('medal_type').cast('int')) \\\n",
    "           .withColumnRenamed('medal_type', 'medal_id') \\\n",
    "\n",
    "    #joining table\n",
    "    df = df.join(df_discipline, df['discipline'] == df_discipline['discipline_type'], how = 'left')\n",
    "    df = df.join(df_event, df['event'] == df_event['event_type'], how = 'left')\n",
    "    #select\n",
    "    df_gold = df.select('medallist_id', 'athletes_id', 'medal_id', 'medal_date', 'discipline_id', 'event_id').distinct()\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'fact_medallist', HDFS_load + '/fact_medallist/')\n",
    "\n",
    "#dim table - athletes\n",
    "def dim_athletes(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_athletes_path = \"hdfs://namenode:9000/datalake/silver_storage/athletes_silver\"\n",
    "    df = read_HDFS(spark, HDFS_athletes_path)\n",
    "\n",
    "    #select\n",
    "    df = df.select('athletes_id', 'full_name', 'gender', \n",
    "                   'function', 'country_code', 'nationality_code', \n",
    "                   'height', 'weight', 'birth_date').distinct()\n",
    "    #rename\n",
    "    df_gold = df.withColumnRenamed('country_code', 'country_id') \\\n",
    "                .withColumnRenamed('nationality_code', 'nationality_id')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_athletes', HDFS_load + '/dim_athletes/')\n",
    "\n",
    "#fact table - medal_team\n",
    "def fact_medal_team(spark: SparkSession, HDFS_load):\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/medals_silver\"\n",
    "    HDFS_discipline_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_discipline\"\n",
    "    HDFS_event_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_event\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df_discipline = read_HDFS(spark, HDFS_discipline_gold)\n",
    "    df_event = read_HDFS(spark, HDFS_event_gold)\n",
    "    \n",
    "    df = df.filter(length('athletes_id') > 7)\n",
    "    df = df.select('athletes_id', 'medal_type', 'medal_date', 'discipline', 'event') \\\n",
    "           .withColumnRenamed('athletes_id', 'team_id')\n",
    "    #handle table individually\n",
    "    df = df.withColumn('medal_team_id', concat(col('team_id'),substring('medal_type',1,1))) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Bronze Medal', '3')) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Silver Medal', '2')) \\\n",
    "           .withColumn('medal_type', regexp_replace('medal_type', 'Gold Medal', '1')) \\\n",
    "           .withColumn('medal_type', col('medal_type').cast('int')) \\\n",
    "           .withColumnRenamed('medal_type', 'medal_id') \\\n",
    "\n",
    "    #joining table\n",
    "    df = df.join(df_discipline, df['discipline'] == df_discipline['discipline_type'], how = 'left')\n",
    "    df = df.join(df_event, df['event'] == df_event['event_type'], how = 'left')\n",
    "    #select\n",
    "    df_gold = df.select('medal_team_id', 'team_id', 'medal_id', 'medal_date', 'discipline_id', 'event_id').distinct()\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'fact_medal_team', HDFS_load + '/fact_medal_team/')\n",
    "    \n",
    "#dim table - team\n",
    "def dim_team(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/teams_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df_gold = df.select('team_id', 'team_name', 'team_gender').distinct()\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold, 'dim_team', HDFS_load + '/dim_team/')\n",
    "\n",
    "#dim table - athletes-team (associating dimension table)\n",
    "def dim_athletes_team(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/silver_storage/teams_silver\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df_gold = df.select('athletes_id', 'team_id')\n",
    "    upload_HDFS(df_gold, 'dim_athletes_team', HDFS_load + '/dim_athletes_team/')\n",
    "\n",
    "#fact table - schedule & dim table - venue\n",
    "def fact_schedule_dim_venue(spark: SparkSession, HDFS_load):\n",
    "    #hdfs path\n",
    "    HDFS_schedule_path = \"hdfs://namenode:9000/datalake/silver_storage/schedules_silver\"\n",
    "    HDFS_venue_path = \"hdfs://namenode:9000/datalake/silver_storage/venues_silver\"\n",
    "    HDFS_discipline_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_discipline\"\n",
    "    HDFS_event_gold = \"hdfs://namenode:9000/datalake/gold_storage/dim_event\"\n",
    "\n",
    "    df = read_HDFS(spark, HDFS_schedule_path)\n",
    "    df_discipline = read_HDFS(spark, HDFS_discipline_gold)\n",
    "    df_event = read_HDFS(spark, HDFS_event_gold)\n",
    "    df_venue = read_HDFS(spark, HDFS_venue_path)\n",
    "    \n",
    "    '''\n",
    "        Create fact - schedule table\n",
    "    '''\n",
    "    df = df.withColumn('schedule_id', monotonically_increasing_id())\n",
    "    df = df.withColumn('schedule_id', concat(lit('sched'), col('schedule_id'))) \\\n",
    "    #drop column\n",
    "    df = df.drop('event_type')\n",
    "    #joining table\n",
    "    df = df.join(df_discipline, df['discipline'] == df_discipline['discipline_type'], how = 'left')\n",
    "    df = df.join(df_event, df['event'] == df_event['event_type'], how = 'left')\n",
    "    #select\n",
    "    df_gold_schedule = df.select('schedule_id', 'start_date', 'end_date', \n",
    "                        'gender', 'discipline_id', 'phase', 'venue_code', 'event_id') \\\n",
    "                .withColumnRenamed('venue_code', 'venue_id') \\\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold_schedule, 'fact_schedule', HDFS_load + '/fact_schedule/')\n",
    "\n",
    "    '''\n",
    "        Create dim - venue table\n",
    "    '''\n",
    "    #get data\n",
    "    df_venue_join = df.select('venue_code', 'venue') \\\n",
    "                      .withColumnRenamed('venue', 'venue_join')\n",
    "    df_venue = df_venue.join(df_venue_join, df_venue['venue'] == df_venue_join['venue_join'], how = 'left')\n",
    "    #select\n",
    "    df_gold_venue = df_venue.select('venue_code', 'venue', 'date_start', 'date_end').distinct() \\\n",
    "                       .withColumnRenamed('venue_code', 'venue_id') \\\n",
    "                       .withColumnRenamed('venue', 'venue_name')\n",
    "    #upload hdfs\n",
    "    upload_HDFS(df_gold_venue, 'dim_venue', HDFS_load + '/dim_venue/')\n",
    "\n",
    "#main call\n",
    "def gold_task(spark, HDFS_load):\n",
    "    #all func\n",
    "    dim_medal(spark, HDFS_load)\n",
    "    dim_discipline(spark, HDFS_load)\n",
    "    dim_event(spark, HDFS_load)\n",
    "    dim_country(spark, HDFS_load)\n",
    "    fact_medallist(spark, HDFS_load)\n",
    "    dim_athletes(spark, HDFS_load)\n",
    "    fact_medal_team(spark, HDFS_load)\n",
    "    fact_schedule_dim_venue(spark, HDFS_load)\n",
    "    dim_team(spark, HDFS_load)\n",
    "    dim_athletes_team(spark, HDFS_load)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #hdfs path\n",
    "    HDFS_load = \"hdfs://namenode:9000/datalake/gold_storage\"\n",
    "\n",
    "    print(\"=========================Gold task starts!========================\")\n",
    "    with get_sparkSession(\"gold_task_spark\", \"local\") as spark:\n",
    "        gold_task(spark, HDFS_load)\n",
    "    print(\"========================Gold task finishes!========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968b76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|medal_id|  medal_type|\n",
      "+--------+------------+\n",
      "|       1|  Gold Medal|\n",
      "|       2|Silver Medal|\n",
      "|       3|Bronze Medal|\n",
      "+--------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 15:51:38 WARN ServerConnection$: JDBC 3.19.0 is being used. But the certified JDBC version 3.13.30 is recommended.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"MySparkApp\") \\\n",
    "        .master(\"local[*]\")\\\n",
    "        .config('spark.jars','/opt/jars/snowflake-jdbc-3.19.0.jar, /opt/jars/spark-snowflake_2.12-2.12.0-spark_3.4.jar')\\\n",
    "        .getOrCreate()\n",
    "sfOptions = {\n",
    "    \"sfURL\": \"https://ae58556.ap-southeast-1.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"HUYNHTHUAN\",\n",
    "    \"sfPassword\": \"Thuan0355389551\",\n",
    "    \"sfDatabase\": \"OLYMPICS_DB\",\n",
    "    \"sfSchema\": \"OLYMPICS_SCHEMA\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "df = spark.read.parquet(\"hdfs://namenode:9000/datalake/gold_storage/dim_medal\", header = True)\n",
    "df.show()\n",
    "df.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"DIM_MEDAL\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b82542-76db-4ecf-a0ab-c2ed63361a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully create SparkSession with app name: check, master: local\n",
      "\n",
      "Starting read file from hdfs://namenode:9000/datalake/gold_storage/dim_country.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting read file from hdfs://namenode:9000/datalake/silver_storage/teams_silver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----------+-----------+--------------------+------------+----------+----------------+--------------------+------------+--------------------+-----------+\n",
      "|          team_id|           team_name|team_gender|    country|        country_full|country_code|discipline|disciplines_code|               event|num_athletes|            athletes|athletes_id|\n",
      "+-----------------+--------------------+-----------+-----------+--------------------+------------+----------+----------------+--------------------+------------+--------------------+-----------+\n",
      "|ATHX4X400M--SUI01|         Switzerland|          X|Switzerland|         Switzerland|         SUI| Athletics|             ATH|4 x 400m Relay Mixed|           6|    DEVANTAY Charles|    1976491|\n",
      "|ATHX4X400M--SUI01|         Switzerland|          X|Switzerland|         Switzerland|         SUI| Athletics|             ATH|4 x 400m Relay Mixed|           6|   PETRUCCIANI Ricky|    1977778|\n",
      "|ATHX4X400M--SUI01|         Switzerland|          X|Switzerland|         Switzerland|         SUI| Athletics|             ATH|4 x 400m Relay Mixed|           6|        SPITZ Lionel|    1977868|\n",
      "|ATHX4X400M--SUI01|         Switzerland|          X|Switzerland|         Switzerland|         SUI| Athletics|             ATH|4 x 400m Relay Mixed|           6|        GIGER Yasmin|    1976537|\n",
      "|ATHX4X400M--SUI01|         Switzerland|          X|Switzerland|         Switzerland|         SUI| Athletics|             ATH|4 x 400m Relay Mixed|           6|  NIEDERBERGER Julia|    1977773|\n",
      "|ATHX4X400M--SUI01|         Switzerland|          X|Switzerland|         Switzerland|         SUI| Athletics|             ATH|4 x 400m Relay Mixed|           6|         SENN Giulia|    1977861|\n",
      "|ROWMSCULL4--EST01|             Estonia|          M|    Estonia|             Estonia|         EST|    Rowing|             ROW|Men's Quadruple S...|           4|    KUSHTEYN Mikhail|    1910465|\n",
      "|ROWMSCULL4--EST01|             Estonia|          M|    Estonia|             Estonia|         EST|    Rowing|             ROW|Men's Quadruple S...|           4|          RAJA Allar|    1910468|\n",
      "|ROWMSCULL4--EST01|             Estonia|          M|    Estonia|             Estonia|         EST|    Rowing|             ROW|Men's Quadruple S...|           4|      ENDREKSON Tonu|    1910471|\n",
      "|ROWMSCULL4--EST01|             Estonia|          M|    Estonia|             Estonia|         EST|    Rowing|             ROW|Men's Quadruple S...|           4|       POOLAK Johann|    1910481|\n",
      "|ROWWSCULL2-LCHN01|People's Republic...|          W|      China|People's Republic...|         CHN|    Rowing|             ROW|Lightweight Women...|           2|           ZOU Jiaqi|    1918420|\n",
      "|ROWWSCULL2-LCHN01|People's Republic...|          W|      China|People's Republic...|         CHN|    Rowing|             ROW|Lightweight Women...|           2|         QIU Xiuping|    1918419|\n",
      "|SWMM4X200MFRBRA01|              Brazil|          M|     Brazil|              Brazil|         BRA|  Swimming|             SWM|Men's 4 x 200m Fr...|           4|SETIN SARTORI Murilo|    1954380|\n",
      "|SWMM4X200MFRBRA01|              Brazil|          M|     Brazil|              Brazil|         BRA|  Swimming|             SWM|Men's 4 x 200m Fr...|           4|   SCHEFFER Fernando|    3430817|\n",
      "|SWMM4X200MFRBRA01|              Brazil|          M|     Brazil|              Brazil|         BRA|  Swimming|             SWM|Men's 4 x 200m Fr...|           4|OLIVEIRA de MORAE...|    1954319|\n",
      "|SWMM4X200MFRBRA01|              Brazil|          M|     Brazil|              Brazil|         BRA|  Swimming|             SWM|Men's 4 x 200m Fr...|           4|     COSTA Guilherme|    1951604|\n",
      "|SWMM4X200MFRJPN01|               Japan|          M|      Japan|               Japan|         JPN|  Swimming|             SWM|Men's 4 x 200m Fr...|           4|      MURASA Tatsuya|    1963632|\n",
      "|SWMM4X200MFRJPN01|               Japan|          M|      Japan|               Japan|         JPN|  Swimming|             SWM|Men's 4 x 200m Fr...|           4| MATSUMOTO Katsuhiro|    1891652|\n",
      "|SWMM4X200MFRJPN01|               Japan|          M|      Japan|               Japan|         JPN|  Swimming|             SWM|Men's 4 x 200m Fr...|           4|       MANO Hidenari|    1891618|\n",
      "|SWMM4X200MFRJPN01|               Japan|          M|      Japan|               Japan|         JPN|  Swimming|             SWM|Men's 4 x 200m Fr...|           4| YANAGIMOTO Konosuke|    1963640|\n",
      "+-----------------+--------------------+-----------+-----------+--------------------+------------+----------+----------------+--------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with get_sparkSession('check') as spark:\n",
    "    HDFS_path = \"hdfs://namenode:9000/datalake/gold_storage/dim_country\"\n",
    "    df = read_HDFS(spark, HDFS_path)\n",
    "    df2 = read_HDFS(spark,\"hdfs://namenode:9000/datalake/silver_storage/teams_silver\")\n",
    "    # #df.show()\n",
    "    # #df2.select('country_code').show()\n",
    "    # df2 = df2.join(df, df['country_id'] == df2['country_code'], how = 'left')\n",
    "    # df2.select('country_code', 'country_id').filter(col('country_id').isNull()).show()\n",
    "    df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede41b7-04b0-4e3e-a634-59ba3415a61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
